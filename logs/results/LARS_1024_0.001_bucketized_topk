2020-03-25 13:03:48,429 [dist_trainer.py:135] INFO Configurations: Namespace(batch_size=1024, compressor='topk', data_dir='/home/sbhatt/dlcom/codebase/gtopk_sgd_modified/data', dataset='cifar10', density=0.001, dnn='resnet20', lr=0.1, max_epochs=141, nsteps_update=1, num_steps=35, nworkers=2, nwpernode=1, pretrain=None, saved_dir='./logs/iclr', threshold=524288000)
2020-03-25 13:03:51,471 [dl_trainer.py:254] INFO num_batches_per_epoch: 25
2020-03-25 13:03:51,687 [distributed_optimizer.py:63] INFO _dynamic_densities: [0.015625, 0.004, 0.001]
2020-03-25 13:03:51,689 [distributed_optimizer.py:323] INFO # of parameters: 269722
2020-03-25 13:03:51,689 [distributed_optimizer.py:324] INFO Total number of tensors: 59
2020-03-25 13:03:51,689 [distributed_optimizer.py:325] INFO Merged Number of groups: 1
2020-03-25 13:03:52,113 [dist_trainer.py:62] INFO max_epochs: 141
2020-03-25 13:04:12,209 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.837252, Speed: 1223.049112 images/s
2020-03-25 13:04:12,211 [distributed_optimizer.py:142] INFO Average number of selected gradients: 4214.000000, exact k: 1078
2020-03-25 13:04:12,211 [distributed_optimizer.py:143] INFO The number of selected gradients: [4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214, 4214]
2020-03-25 13:04:13,146 [dl_trainer.py:634] INFO train iter: 25, num_batches_per_epoch: 25
2020-03-25 13:04:13,146 [dl_trainer.py:635] INFO Epoch 1, avg train acc: 10.082031, lr: 0.020640, avg loss: 5.266054
2020-03-25 13:04:16,008 [dl_trainer.py:822] INFO Epoch 1, lr: 0.020640, val loss: 10.093372, val top-1 acc: 6.140984, top-5 acc: 48.330277
2020-03-25 13:04:28,167 [dl_trainer.py:732] WARNING [  1][   40/   25][rank:0] loss: 4.641, average forward (0.015918) and backward (0.468042) time: 0.808583, iotime: 0.252523 
2020-03-25 13:04:34,832 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.942503, Speed: 1086.468730 images/s
2020-03-25 13:04:34,832 [distributed_optimizer.py:142] INFO Average number of selected gradients: 1078.000000, exact k: 269
2020-03-25 13:04:34,833 [distributed_optimizer.py:143] INFO The number of selected gradients: [1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078]
2020-03-25 13:04:36,575 [dl_trainer.py:634] INFO train iter: 50, num_batches_per_epoch: 25
2020-03-25 13:04:36,576 [dl_trainer.py:635] INFO Epoch 2, avg train acc: 10.460938, lr: 0.040480, avg loss: 5.027429
2020-03-25 13:04:39,442 [dl_trainer.py:822] INFO Epoch 2, lr: 0.040480, val loss: 10.621503, val top-1 acc: 13.110850, top-5 acc: 54.463887
2020-03-25 13:04:57,463 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.942930, Speed: 1085.977190 images/s
2020-03-25 13:04:57,464 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:04:57,464 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:05:00,043 [dl_trainer.py:634] INFO train iter: 75, num_batches_per_epoch: 25
2020-03-25 13:05:00,044 [dl_trainer.py:635] INFO Epoch 3, avg train acc: 11.226562, lr: 0.060320, avg loss: 4.012672
2020-03-25 13:05:02,902 [dl_trainer.py:822] INFO Epoch 3, lr: 0.060320, val loss: 4.341897, val top-1 acc: 12.394172, top-5 acc: 53.264907
2020-03-25 13:05:06,903 [dl_trainer.py:732] WARNING [  3][   80/   25][rank:0] loss: 4.413, average forward (0.004313) and backward (0.474658) time: 0.874998, iotime: 0.251945 
2020-03-25 13:05:20,163 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.945764, Speed: 1082.722113 images/s
2020-03-25 13:05:20,164 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:05:20,164 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:05:23,560 [dl_trainer.py:634] INFO train iter: 100, num_batches_per_epoch: 25
2020-03-25 13:05:23,560 [dl_trainer.py:635] INFO Epoch 4, avg train acc: 11.437500, lr: 0.080160, avg loss: 4.145328
2020-03-25 13:05:26,410 [dl_trainer.py:822] INFO Epoch 4, lr: 0.080160, val loss: 5.377874, val top-1 acc: 12.465521, top-5 acc: 51.101124
2020-03-25 13:05:42,723 [dl_trainer.py:732] WARNING [  4][  120/   25][rank:0] loss: 4.219, average forward (0.004278) and backward (0.478644) time: 0.800487, iotime: 0.245494 
2020-03-25 13:05:42,867 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.945956, Speed: 1082.502585 images/s
2020-03-25 13:05:42,867 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:05:42,868 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:05:47,098 [dl_trainer.py:634] INFO train iter: 125, num_batches_per_epoch: 25
2020-03-25 13:05:47,099 [dl_trainer.py:635] INFO Epoch 5, avg train acc: 11.742188, lr: 0.100000, avg loss: 4.570124
2020-03-25 13:05:49,956 [dl_trainer.py:822] INFO Epoch 5, lr: 0.100000, val loss: 6.911103, val top-1 acc: 12.253667, top-5 acc: 52.363082
2020-03-25 13:06:05,568 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.945848, Speed: 1082.626830 images/s
2020-03-25 13:06:05,569 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:06:05,569 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:06:10,647 [dl_trainer.py:634] INFO train iter: 150, num_batches_per_epoch: 25
2020-03-25 13:06:10,647 [dl_trainer.py:635] INFO Epoch 6, avg train acc: 12.058594, lr: 0.100000, avg loss: 5.047386
2020-03-25 13:06:13,502 [dl_trainer.py:822] INFO Epoch 6, lr: 0.100000, val loss: 6.630135, val top-1 acc: 12.586495, top-5 acc: 53.877152
2020-03-25 13:06:21,612 [dl_trainer.py:732] WARNING [  6][  160/   25][rank:0] loss: 4.675, average forward (0.004259) and backward (0.480117) time: 0.871987, iotime: 0.244093 
2020-03-25 13:06:28,319 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.947905, Speed: 1080.277514 images/s
2020-03-25 13:06:28,320 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:06:28,320 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:06:34,210 [dl_trainer.py:634] INFO train iter: 175, num_batches_per_epoch: 25
2020-03-25 13:06:34,210 [dl_trainer.py:635] INFO Epoch 7, avg train acc: 11.726562, lr: 0.100000, avg loss: 4.983042
2020-03-25 13:06:37,059 [dl_trainer.py:822] INFO Epoch 7, lr: 0.100000, val loss: 6.393484, val top-1 acc: 12.039620, top-5 acc: 54.423230
2020-03-25 13:06:51,020 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.945819, Speed: 1082.660113 images/s
2020-03-25 13:06:51,020 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:06:51,021 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:06:57,648 [dl_trainer.py:732] WARNING [  7][  200/   25][rank:0] loss: 6.139, average forward (0.004269) and backward (0.480120) time: 0.800159, iotime: 0.244084 
2020-03-25 13:06:57,744 [dl_trainer.py:634] INFO train iter: 200, num_batches_per_epoch: 25
2020-03-25 13:06:57,744 [dl_trainer.py:635] INFO Epoch 8, avg train acc: 11.765625, lr: 0.100000, avg loss: 5.097348
2020-03-25 13:07:00,620 [dl_trainer.py:822] INFO Epoch 8, lr: 0.100000, val loss: 6.282727, val top-1 acc: 12.582709, top-5 acc: 56.063855
2020-03-25 13:07:13,865 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.951850, Speed: 1075.800076 images/s
2020-03-25 13:07:13,866 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:07:13,866 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:07:21,410 [dl_trainer.py:634] INFO train iter: 225, num_batches_per_epoch: 25
2020-03-25 13:07:21,410 [dl_trainer.py:635] INFO Epoch 9, avg train acc: 10.824219, lr: 0.100000, avg loss: 5.211904
2020-03-25 13:07:24,263 [dl_trainer.py:822] INFO Epoch 9, lr: 0.100000, val loss: 5.802234, val top-1 acc: 11.339684, top-5 acc: 52.339764
2020-03-25 13:07:36,475 [dl_trainer.py:732] WARNING [  9][  240/   25][rank:0] loss: 5.279, average forward (0.004252) and backward (0.480997) time: 0.871253, iotime: 0.241978 
2020-03-25 13:07:36,577 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.946298, Speed: 1082.111653 images/s
2020-03-25 13:07:36,578 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:07:36,578 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:07:44,956 [dl_trainer.py:634] INFO train iter: 250, num_batches_per_epoch: 25
2020-03-25 13:07:44,957 [dl_trainer.py:635] INFO Epoch 10, avg train acc: 11.832031, lr: 0.100000, avg loss: 4.801557
2020-03-25 13:07:47,830 [dl_trainer.py:822] INFO Epoch 10, lr: 0.100000, val loss: 5.012539, val top-1 acc: 12.013313, top-5 acc: 55.243543
2020-03-25 13:07:59,370 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.949656, Speed: 1078.285156 images/s
2020-03-25 13:07:59,371 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:07:59,371 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:08:08,575 [dl_trainer.py:634] INFO train iter: 275, num_batches_per_epoch: 25
2020-03-25 13:08:08,576 [dl_trainer.py:635] INFO Epoch 11, avg train acc: 11.367188, lr: 0.100000, avg loss: 4.570848
2020-03-25 13:08:11,427 [dl_trainer.py:822] INFO Epoch 11, lr: 0.100000, val loss: 5.799398, val top-1 acc: 12.843192, top-5 acc: 53.416175
2020-03-25 13:08:15,387 [dl_trainer.py:732] WARNING [ 11][  280/   25][rank:0] loss: 6.682, average forward (0.004288) and backward (0.480905) time: 0.873783, iotime: 0.244586 
2020-03-25 13:08:22,103 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.947159, Speed: 1081.128000 images/s
2020-03-25 13:08:22,104 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:08:22,104 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:08:32,122 [dl_trainer.py:634] INFO train iter: 300, num_batches_per_epoch: 25
2020-03-25 13:08:32,123 [dl_trainer.py:635] INFO Epoch 12, avg train acc: 11.324219, lr: 0.100000, avg loss: 5.090736
2020-03-25 13:08:34,980 [dl_trainer.py:822] INFO Epoch 12, lr: 0.100000, val loss: 5.246050, val top-1 acc: 10.893455, top-5 acc: 53.891701
2020-03-25 13:08:44,910 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.950246, Speed: 1077.615278 images/s
2020-03-25 13:08:44,911 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:08:44,911 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:08:51,562 [dl_trainer.py:732] WARNING [ 12][  320/   25][rank:0] loss: 4.488, average forward (0.004268) and backward (0.481581) time: 0.804634, iotime: 0.246469 
2020-03-25 13:08:55,791 [dl_trainer.py:634] INFO train iter: 325, num_batches_per_epoch: 25
2020-03-25 13:08:55,792 [dl_trainer.py:635] INFO Epoch 13, avg train acc: 11.246094, lr: 0.100000, avg loss: 4.645907
2020-03-25 13:08:58,641 [dl_trainer.py:822] INFO Epoch 13, lr: 0.100000, val loss: 5.784242, val top-1 acc: 11.583626, top-5 acc: 56.239637
2020-03-25 13:09:07,646 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.947308, Speed: 1080.958098 images/s
2020-03-25 13:09:07,647 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:09:07,647 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:09:19,343 [dl_trainer.py:634] INFO train iter: 350, num_batches_per_epoch: 25
2020-03-25 13:09:19,344 [dl_trainer.py:635] INFO Epoch 14, avg train acc: 11.597656, lr: 0.100000, avg loss: 4.760656
2020-03-25 13:09:22,197 [dl_trainer.py:822] INFO Epoch 14, lr: 0.100000, val loss: 5.250987, val top-1 acc: 10.867945, top-5 acc: 52.757494
2020-03-25 13:09:30,311 [dl_trainer.py:732] WARNING [ 14][  360/   25][rank:0] loss: 4.464, average forward (0.004225) and backward (0.481430) time: 0.870479, iotime: 0.241531 
2020-03-25 13:09:30,415 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.948665, Speed: 1079.411986 images/s
2020-03-25 13:09:30,416 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:09:30,416 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:09:42,923 [dl_trainer.py:634] INFO train iter: 375, num_batches_per_epoch: 25
2020-03-25 13:09:42,924 [dl_trainer.py:635] INFO Epoch 15, avg train acc: 11.070312, lr: 0.100000, avg loss: 4.576102
2020-03-25 13:09:45,771 [dl_trainer.py:822] INFO Epoch 15, lr: 0.100000, val loss: 5.253096, val top-1 acc: 11.562699, top-5 acc: 52.887237
2020-03-25 13:09:53,177 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.948373, Speed: 1079.744416 images/s
2020-03-25 13:09:53,178 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:09:53,178 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:10:06,465 [dl_trainer.py:732] WARNING [ 15][  400/   25][rank:0] loss: 5.505, average forward (0.004221) and backward (0.481832) time: 0.802290, iotime: 0.244628 
2020-03-25 13:10:06,563 [dl_trainer.py:634] INFO train iter: 400, num_batches_per_epoch: 25
2020-03-25 13:10:06,563 [dl_trainer.py:635] INFO Epoch 16, avg train acc: 8.652344, lr: 0.100000, avg loss: 4.314454
2020-03-25 13:10:09,419 [dl_trainer.py:822] INFO Epoch 16, lr: 0.100000, val loss: 5.488821, val top-1 acc: 8.807398, top-5 acc: 52.999841
2020-03-25 13:10:16,021 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.951809, Speed: 1075.846078 images/s
2020-03-25 13:10:16,022 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:10:16,022 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:10:30,281 [dl_trainer.py:634] INFO train iter: 425, num_batches_per_epoch: 25
2020-03-25 13:10:30,282 [dl_trainer.py:635] INFO Epoch 17, avg train acc: 10.097656, lr: 0.100000, avg loss: 4.810582
2020-03-25 13:10:33,139 [dl_trainer.py:822] INFO Epoch 17, lr: 0.100000, val loss: 7.013118, val top-1 acc: 11.350845, top-5 acc: 52.164780
2020-03-25 13:10:38,890 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.952816, Speed: 1074.709342 images/s
2020-03-25 13:10:38,891 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:10:38,891 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:10:45,549 [dl_trainer.py:732] WARNING [ 17][  440/   25][rank:0] loss: 3.901, average forward (0.004246) and backward (0.481663) time: 0.873347, iotime: 0.243877 
2020-03-25 13:10:53,963 [dl_trainer.py:634] INFO train iter: 450, num_batches_per_epoch: 25
2020-03-25 13:10:53,963 [dl_trainer.py:635] INFO Epoch 18, avg train acc: 11.097656, lr: 0.100000, avg loss: 4.709912
2020-03-25 13:10:56,812 [dl_trainer.py:822] INFO Epoch 18, lr: 0.100000, val loss: 4.902124, val top-1 acc: 10.921158, top-5 acc: 49.597218
2020-03-25 13:11:01,740 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.952014, Speed: 1075.614413 images/s
2020-03-25 13:11:01,740 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:11:01,740 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:11:17,734 [dl_trainer.py:634] INFO train iter: 475, num_batches_per_epoch: 25
2020-03-25 13:11:17,734 [dl_trainer.py:635] INFO Epoch 19, avg train acc: 11.335938, lr: 0.100000, avg loss: 4.569284
2020-03-25 13:11:20,593 [dl_trainer.py:822] INFO Epoch 19, lr: 0.100000, val loss: 5.212903, val top-1 acc: 11.767777, top-5 acc: 51.766382
2020-03-25 13:11:24,636 [dl_trainer.py:732] WARNING [ 19][  480/   25][rank:0] loss: 3.606, average forward (0.004384) and backward (0.480719) time: 0.879985, iotime: 0.251422 
2020-03-25 13:11:24,732 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.957961, Speed: 1068.936542 images/s
2020-03-25 13:11:24,732 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:11:24,732 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:11:41,458 [dl_trainer.py:634] INFO train iter: 500, num_batches_per_epoch: 25
2020-03-25 13:11:41,458 [dl_trainer.py:635] INFO Epoch 20, avg train acc: 11.613281, lr: 0.100000, avg loss: 4.445483
2020-03-25 13:11:44,338 [dl_trainer.py:822] INFO Epoch 20, lr: 0.100000, val loss: 4.557566, val top-1 acc: 10.849211, top-5 acc: 51.355827
2020-03-25 13:11:47,662 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.955414, Speed: 1071.786149 images/s
2020-03-25 13:11:47,663 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:11:47,663 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:12:00,915 [dl_trainer.py:732] WARNING [ 20][  520/   25][rank:0] loss: 4.658, average forward (0.004237) and backward (0.481257) time: 0.804472, iotime: 0.246287 
2020-03-25 13:12:05,213 [dl_trainer.py:634] INFO train iter: 525, num_batches_per_epoch: 25
2020-03-25 13:12:05,214 [dl_trainer.py:635] INFO Epoch 21, avg train acc: 11.171875, lr: 0.100000, avg loss: 4.687664
2020-03-25 13:12:08,063 [dl_trainer.py:822] INFO Epoch 21, lr: 0.100000, val loss: 3.831684, val top-1 acc: 11.398876, top-5 acc: 54.085220
2020-03-25 13:12:10,477 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.950580, Speed: 1077.236536 images/s
2020-03-25 13:12:10,478 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:12:10,478 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:12:28,888 [dl_trainer.py:634] INFO train iter: 550, num_batches_per_epoch: 25
2020-03-25 13:12:28,889 [dl_trainer.py:635] INFO Epoch 22, avg train acc: 10.972656, lr: 0.100000, avg loss: 4.365302
2020-03-25 13:12:31,740 [dl_trainer.py:822] INFO Epoch 22, lr: 0.100000, val loss: 5.135324, val top-1 acc: 11.594188, top-5 acc: 55.439054
2020-03-25 13:12:33,341 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.952599, Speed: 1074.954283 images/s
2020-03-25 13:12:33,341 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:12:33,341 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:12:39,984 [dl_trainer.py:732] WARNING [ 22][  560/   25][rank:0] loss: 3.366, average forward (0.004212) and backward (0.481235) time: 0.872011, iotime: 0.243319 
2020-03-25 13:12:52,554 [dl_trainer.py:634] INFO train iter: 575, num_batches_per_epoch: 25
2020-03-25 13:12:52,555 [dl_trainer.py:635] INFO Epoch 23, avg train acc: 11.769531, lr: 0.100000, avg loss: 4.491705
2020-03-25 13:12:55,403 [dl_trainer.py:822] INFO Epoch 23, lr: 0.100000, val loss: 4.359264, val top-1 acc: 11.730309, top-5 acc: 52.427655
2020-03-25 13:12:56,157 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.950633, Speed: 1077.177279 images/s
2020-03-25 13:12:56,157 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:12:56,157 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:13:16,158 [dl_trainer.py:732] WARNING [ 23][  600/   25][rank:0] loss: 4.148, average forward (0.004248) and backward (0.480965) time: 0.798169, iotime: 0.241334 
2020-03-25 13:13:16,264 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.837747, Speed: 1222.326452 images/s
2020-03-25 13:13:16,264 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:13:16,264 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:13:16,265 [dl_trainer.py:634] INFO train iter: 600, num_batches_per_epoch: 25
2020-03-25 13:13:16,265 [dl_trainer.py:635] INFO Epoch 24, avg train acc: 10.425781, lr: 0.100000, avg loss: 4.267740
2020-03-25 13:13:19,116 [dl_trainer.py:822] INFO Epoch 24, lr: 0.100000, val loss: 4.936678, val top-1 acc: 11.495735, top-5 acc: 54.686304
2020-03-25 13:13:39,188 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.955144, Speed: 1072.089210 images/s
2020-03-25 13:13:39,188 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:13:39,189 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:13:40,139 [dl_trainer.py:634] INFO train iter: 625, num_batches_per_epoch: 25
2020-03-25 13:13:40,139 [dl_trainer.py:635] INFO Epoch 25, avg train acc: 11.046875, lr: 0.100000, avg loss: 4.471938
2020-03-25 13:13:42,984 [dl_trainer.py:822] INFO Epoch 25, lr: 0.100000, val loss: 5.234921, val top-1 acc: 10.998485, top-5 acc: 51.052296
2020-03-25 13:13:55,239 [dl_trainer.py:732] WARNING [ 25][  640/   25][rank:0] loss: 3.880, average forward (0.004200) and backward (0.481619) time: 0.871852, iotime: 0.242961 
2020-03-25 13:14:01,976 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.949466, Speed: 1078.500686 images/s
2020-03-25 13:14:01,977 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:14:01,977 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:14:03,763 [dl_trainer.py:634] INFO train iter: 650, num_batches_per_epoch: 25
2020-03-25 13:14:03,764 [dl_trainer.py:635] INFO Epoch 26, avg train acc: 10.839844, lr: 0.100000, avg loss: 4.599116
2020-03-25 13:14:06,610 [dl_trainer.py:822] INFO Epoch 26, lr: 0.100000, val loss: 5.842096, val top-1 acc: 9.075654, top-5 acc: 50.362325
2020-03-25 13:14:24,868 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.953789, Speed: 1073.613058 images/s
2020-03-25 13:14:24,869 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:14:24,869 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:14:27,452 [dl_trainer.py:634] INFO train iter: 675, num_batches_per_epoch: 25
2020-03-25 13:14:27,453 [dl_trainer.py:635] INFO Epoch 27, avg train acc: 10.523438, lr: 0.100000, avg loss: 4.213230
2020-03-25 13:14:30,308 [dl_trainer.py:822] INFO Epoch 27, lr: 0.100000, val loss: 4.405319, val top-1 acc: 10.911392, top-5 acc: 51.789302
2020-03-25 13:14:34,305 [dl_trainer.py:732] WARNING [ 27][  680/   25][rank:0] loss: 5.546, average forward (0.004262) and backward (0.480655) time: 0.875475, iotime: 0.247073 
2020-03-25 13:14:47,769 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.954143, Speed: 1073.214507 images/s
2020-03-25 13:14:47,769 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:14:47,769 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:14:51,213 [dl_trainer.py:634] INFO train iter: 700, num_batches_per_epoch: 25
2020-03-25 13:14:51,213 [dl_trainer.py:635] INFO Epoch 28, avg train acc: 11.062500, lr: 0.100000, avg loss: 4.435486
2020-03-25 13:14:54,063 [dl_trainer.py:822] INFO Epoch 28, lr: 0.100000, val loss: 5.627634, val top-1 acc: 11.138393, top-5 acc: 52.069715
2020-03-25 13:15:10,568 [dl_trainer.py:732] WARNING [ 28][  720/   25][rank:0] loss: 5.163, average forward (0.004229) and backward (0.479865) time: 0.798262, iotime: 0.241996 
2020-03-25 13:15:10,670 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.954171, Speed: 1073.182942 images/s
2020-03-25 13:15:10,670 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:15:10,670 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:15:14,933 [dl_trainer.py:634] INFO train iter: 725, num_batches_per_epoch: 25
2020-03-25 13:15:14,934 [dl_trainer.py:635] INFO Epoch 29, avg train acc: 10.542969, lr: 0.100000, avg loss: 4.643703
2020-03-25 13:15:17,788 [dl_trainer.py:822] INFO Epoch 29, lr: 0.100000, val loss: 4.547461, val top-1 acc: 10.885284, top-5 acc: 51.216319
2020-03-25 13:15:33,483 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.950526, Speed: 1077.298683 images/s
2020-03-25 13:15:33,484 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:15:33,484 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:15:38,571 [dl_trainer.py:634] INFO train iter: 750, num_batches_per_epoch: 25
2020-03-25 13:15:38,572 [dl_trainer.py:635] INFO Epoch 30, avg train acc: 10.832031, lr: 0.100000, avg loss: 4.298997
2020-03-25 13:15:41,424 [dl_trainer.py:822] INFO Epoch 30, lr: 0.100000, val loss: 4.476675, val top-1 acc: 10.806960, top-5 acc: 53.896684
2020-03-25 13:15:49,614 [dl_trainer.py:732] WARNING [ 30][  760/   25][rank:0] loss: 4.995, average forward (0.004249) and backward (0.480521) time: 0.874181, iotime: 0.245858 
2020-03-25 13:15:56,351 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.952780, Speed: 1074.749525 images/s
2020-03-25 13:15:56,351 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:15:56,351 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:16:02,309 [dl_trainer.py:634] INFO train iter: 775, num_batches_per_epoch: 25
2020-03-25 13:16:02,309 [dl_trainer.py:635] INFO Epoch 31, avg train acc: 10.691406, lr: 0.100000, avg loss: 4.743823
2020-03-25 13:16:05,152 [dl_trainer.py:822] INFO Epoch 31, lr: 0.100000, val loss: 4.418868, val top-1 acc: 10.421516, top-5 acc: 52.821269
2020-03-25 13:16:19,251 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.954145, Speed: 1073.212618 images/s
2020-03-25 13:16:19,252 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:16:19,252 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:16:25,905 [dl_trainer.py:732] WARNING [ 31][  800/   25][rank:0] loss: 4.633, average forward (0.004242) and backward (0.481449) time: 0.799876, iotime: 0.242721 
2020-03-25 13:16:26,022 [dl_trainer.py:634] INFO train iter: 800, num_batches_per_epoch: 25
2020-03-25 13:16:26,022 [dl_trainer.py:635] INFO Epoch 32, avg train acc: 10.953125, lr: 0.100000, avg loss: 4.280741
2020-03-25 13:16:28,863 [dl_trainer.py:822] INFO Epoch 32, lr: 0.100000, val loss: 4.084805, val top-1 acc: 10.562221, top-5 acc: 52.209224
2020-03-25 13:16:42,055 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.950105, Speed: 1077.776131 images/s
2020-03-25 13:16:42,056 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:16:42,056 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:16:49,619 [dl_trainer.py:634] INFO train iter: 825, num_batches_per_epoch: 25
2020-03-25 13:16:49,619 [dl_trainer.py:635] INFO Epoch 33, avg train acc: 10.390625, lr: 0.100000, avg loss: 4.363038
2020-03-25 13:16:52,488 [dl_trainer.py:822] INFO Epoch 33, lr: 0.100000, val loss: 5.771543, val top-1 acc: 11.462054, top-5 acc: 54.696867
2020-03-25 13:17:04,743 [dl_trainer.py:732] WARNING [ 33][  840/   25][rank:0] loss: 4.811, average forward (0.004234) and backward (0.482058) time: 0.867939, iotime: 0.238257 
2020-03-25 13:17:04,843 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.949445, Speed: 1078.524868 images/s
2020-03-25 13:17:04,843 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:17:04,843 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:17:13,240 [dl_trainer.py:634] INFO train iter: 850, num_batches_per_epoch: 25
2020-03-25 13:17:13,241 [dl_trainer.py:635] INFO Epoch 34, avg train acc: 10.734375, lr: 0.100000, avg loss: 4.562533
2020-03-25 13:17:16,124 [dl_trainer.py:822] INFO Epoch 34, lr: 0.100000, val loss: 3.860429, val top-1 acc: 11.151148, top-5 acc: 53.402025
2020-03-25 13:17:27,659 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.950655, Speed: 1077.152617 images/s
2020-03-25 13:17:27,660 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:17:27,660 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:17:36,870 [dl_trainer.py:634] INFO train iter: 875, num_batches_per_epoch: 25
2020-03-25 13:17:36,870 [dl_trainer.py:635] INFO Epoch 35, avg train acc: 11.097656, lr: 0.100000, avg loss: 4.319494
2020-03-25 13:17:39,750 [dl_trainer.py:822] INFO Epoch 35, lr: 0.100000, val loss: 4.368379, val top-1 acc: 10.968591, top-5 acc: 52.246891
2020-03-25 13:17:43,730 [dl_trainer.py:732] WARNING [ 35][  880/   25][rank:0] loss: 4.971, average forward (0.004229) and backward (0.481012) time: 0.871837, iotime: 0.241819 
2020-03-25 13:17:50,448 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.949481, Speed: 1078.483794 images/s
2020-03-25 13:17:50,448 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:17:50,449 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:18:00,511 [dl_trainer.py:634] INFO train iter: 900, num_batches_per_epoch: 25
2020-03-25 13:18:00,511 [dl_trainer.py:635] INFO Epoch 36, avg train acc: 10.621094, lr: 0.100000, avg loss: 4.423735
2020-03-25 13:18:03,370 [dl_trainer.py:822] INFO Epoch 36, lr: 0.100000, val loss: 4.331035, val top-1 acc: 10.103237, top-5 acc: 50.629783
2020-03-25 13:18:13,235 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.949439, Speed: 1078.531910 images/s
2020-03-25 13:18:13,236 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:18:13,236 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:18:19,863 [dl_trainer.py:732] WARNING [ 36][  920/   25][rank:0] loss: 4.510, average forward (0.004263) and backward (0.481345) time: 0.803391, iotime: 0.245630 
2020-03-25 13:18:24,084 [dl_trainer.py:634] INFO train iter: 925, num_batches_per_epoch: 25
2020-03-25 13:18:24,084 [dl_trainer.py:635] INFO Epoch 37, avg train acc: 10.648438, lr: 0.100000, avg loss: 4.563361
2020-03-25 13:18:26,936 [dl_trainer.py:822] INFO Epoch 37, lr: 0.100000, val loss: 4.804832, val top-1 acc: 9.916095, top-5 acc: 54.150191
2020-03-25 13:18:35,960 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.946802, Speed: 1081.536049 images/s
2020-03-25 13:18:35,960 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:18:35,960 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:18:47,657 [dl_trainer.py:634] INFO train iter: 950, num_batches_per_epoch: 25
2020-03-25 13:18:47,658 [dl_trainer.py:635] INFO Epoch 38, avg train acc: 10.683594, lr: 0.100000, avg loss: 4.398291
2020-03-25 13:18:50,520 [dl_trainer.py:822] INFO Epoch 38, lr: 0.100000, val loss: 3.508877, val top-1 acc: 11.264549, top-5 acc: 52.974729
2020-03-25 13:18:58,602 [dl_trainer.py:732] WARNING [ 38][  960/   25][rank:0] loss: 4.075, average forward (0.004261) and backward (0.479648) time: 0.869872, iotime: 0.242278 
2020-03-25 13:18:58,700 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.947494, Speed: 1080.745279 images/s
2020-03-25 13:18:58,701 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:18:58,701 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:19:11,210 [dl_trainer.py:634] INFO train iter: 975, num_batches_per_epoch: 25
2020-03-25 13:19:11,211 [dl_trainer.py:635] INFO Epoch 39, avg train acc: 10.730469, lr: 0.100000, avg loss: 4.088244
2020-03-25 13:19:14,069 [dl_trainer.py:822] INFO Epoch 39, lr: 0.100000, val loss: 4.148109, val top-1 acc: 11.017419, top-5 acc: 52.026666
2020-03-25 13:19:21,447 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.947728, Speed: 1080.478246 images/s
2020-03-25 13:19:21,447 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:19:21,447 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:19:34,728 [dl_trainer.py:732] WARNING [ 39][ 1000/   25][rank:0] loss: 4.626, average forward (0.004247) and backward (0.479594) time: 0.802181, iotime: 0.246438 
2020-03-25 13:19:34,829 [dl_trainer.py:634] INFO train iter: 1000, num_batches_per_epoch: 25
2020-03-25 13:19:34,829 [dl_trainer.py:635] INFO Epoch 40, avg train acc: 10.453125, lr: 0.100000, avg loss: 4.583181
2020-03-25 13:19:37,688 [dl_trainer.py:822] INFO Epoch 40, lr: 0.100000, val loss: 3.821453, val top-1 acc: 10.266861, top-5 acc: 51.865234
2020-03-25 13:19:44,297 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.952041, Speed: 1075.584390 images/s
2020-03-25 13:19:44,297 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:19:44,298 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:19:58,495 [dl_trainer.py:634] INFO train iter: 1025, num_batches_per_epoch: 25
2020-03-25 13:19:58,495 [dl_trainer.py:635] INFO Epoch 41, avg train acc: 10.410156, lr: 0.100000, avg loss: 4.320137
2020-03-25 13:20:01,351 [dl_trainer.py:822] INFO Epoch 41, lr: 0.100000, val loss: 4.205553, val top-1 acc: 10.082908, top-5 acc: 50.478715
2020-03-25 13:20:07,088 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.949597, Speed: 1078.351755 images/s
2020-03-25 13:20:07,089 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:20:07,089 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:20:13,704 [dl_trainer.py:732] WARNING [ 41][ 1040/   25][rank:0] loss: 4.777, average forward (0.004229) and backward (0.479576) time: 0.872681, iotime: 0.245298 
2020-03-25 13:20:22,099 [dl_trainer.py:634] INFO train iter: 1050, num_batches_per_epoch: 25
2020-03-25 13:20:22,100 [dl_trainer.py:635] INFO Epoch 42, avg train acc: 10.265625, lr: 0.100000, avg loss: 4.425850
2020-03-25 13:20:24,949 [dl_trainer.py:822] INFO Epoch 42, lr: 0.100000, val loss: 3.737101, val top-1 acc: 10.453404, top-5 acc: 52.204639
2020-03-25 13:20:29,873 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.949322, Speed: 1078.664421 images/s
2020-03-25 13:20:29,874 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:20:29,874 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:20:45,627 [dl_trainer.py:634] INFO train iter: 1075, num_batches_per_epoch: 25
2020-03-25 13:20:45,628 [dl_trainer.py:635] INFO Epoch 43, avg train acc: 11.000000, lr: 0.100000, avg loss: 4.039654
2020-03-25 13:20:48,475 [dl_trainer.py:822] INFO Epoch 43, lr: 0.100000, val loss: 3.347226, val top-1 acc: 10.644133, top-5 acc: 51.690051
2020-03-25 13:20:52,429 [dl_trainer.py:732] WARNING [ 43][ 1080/   25][rank:0] loss: 4.290, average forward (0.004245) and backward (0.478543) time: 0.868627, iotime: 0.242727 
2020-03-25 13:20:52,530 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.944007, Speed: 1084.737183 images/s
2020-03-25 13:20:52,531 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:20:52,531 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:21:09,114 [dl_trainer.py:634] INFO train iter: 1100, num_batches_per_epoch: 25
2020-03-25 13:21:09,115 [dl_trainer.py:635] INFO Epoch 44, avg train acc: 10.445312, lr: 0.100000, avg loss: 4.359540
2020-03-25 13:21:11,981 [dl_trainer.py:822] INFO Epoch 44, lr: 0.100000, val loss: 2.812341, val top-1 acc: 10.236169, top-5 acc: 51.334702
2020-03-25 13:21:15,225 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.945596, Speed: 1082.914562 images/s
2020-03-25 13:21:15,226 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:21:15,226 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:21:28,452 [dl_trainer.py:732] WARNING [ 44][ 1120/   25][rank:0] loss: 4.476, average forward (0.004244) and backward (0.478541) time: 0.798790, iotime: 0.243680 
2020-03-25 13:21:32,674 [dl_trainer.py:634] INFO train iter: 1125, num_batches_per_epoch: 25
2020-03-25 13:21:32,674 [dl_trainer.py:635] INFO Epoch 45, avg train acc: 10.394531, lr: 0.100000, avg loss: 4.391255
2020-03-25 13:21:35,521 [dl_trainer.py:822] INFO Epoch 45, lr: 0.100000, val loss: 2.528306, val top-1 acc: 10.491470, top-5 acc: 50.750359
2020-03-25 13:21:37,923 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.945678, Speed: 1082.820621 images/s
2020-03-25 13:21:37,923 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:21:37,923 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:21:56,144 [dl_trainer.py:634] INFO train iter: 1150, num_batches_per_epoch: 25
2020-03-25 13:21:56,145 [dl_trainer.py:635] INFO Epoch 46, avg train acc: 10.472656, lr: 0.100000, avg loss: 3.991581
2020-03-25 13:21:59,018 [dl_trainer.py:822] INFO Epoch 46, lr: 0.100000, val loss: 2.303175, val top-1 acc: 10.010762, top-5 acc: 49.964126
2020-03-25 13:22:00,662 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.947416, Speed: 1080.835074 images/s
2020-03-25 13:22:00,662 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:22:00,662 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:22:07,206 [dl_trainer.py:732] WARNING [ 46][ 1160/   25][rank:0] loss: 3.877, average forward (0.004237) and backward (0.475591) time: 0.869967, iotime: 0.246254 
2020-03-25 13:22:19,475 [dl_trainer.py:634] INFO train iter: 1175, num_batches_per_epoch: 25
2020-03-25 13:22:19,476 [dl_trainer.py:635] INFO Epoch 47, avg train acc: 10.023438, lr: 0.100000, avg loss: 3.278867
2020-03-25 13:22:22,330 [dl_trainer.py:822] INFO Epoch 47, lr: 0.100000, val loss: 2.302764, val top-1 acc: 9.968909, top-5 acc: 49.961137
2020-03-25 13:22:23,070 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.933652, Speed: 1096.768630 images/s
2020-03-25 13:22:23,071 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:22:23,071 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:22:42,414 [dl_trainer.py:732] WARNING [ 47][ 1200/   25][rank:0] loss: 2.302, average forward (0.004254) and backward (0.464280) time: 0.784561, iotime: 0.244208 
2020-03-25 13:22:42,493 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.809241, Speed: 1265.382874 images/s
2020-03-25 13:22:42,493 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:22:42,493 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:22:42,494 [dl_trainer.py:634] INFO train iter: 1200, num_batches_per_epoch: 25
2020-03-25 13:22:42,494 [dl_trainer.py:635] INFO Epoch 48, avg train acc: 9.828125, lr: 0.100000, avg loss: 2.895334
2020-03-25 13:22:45,353 [dl_trainer.py:822] INFO Epoch 48, lr: 0.100000, val loss: 2.302639, val top-1 acc: 10.010762, top-5 acc: 50.017937
2020-03-25 13:23:04,611 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.921557, Speed: 1111.163388 images/s
2020-03-25 13:23:04,612 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:23:04,612 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:23:05,521 [dl_trainer.py:634] INFO train iter: 1225, num_batches_per_epoch: 25
2020-03-25 13:23:05,522 [dl_trainer.py:635] INFO Epoch 49, avg train acc: 9.984375, lr: 0.100000, avg loss: 2.655201
2020-03-25 13:23:08,372 [dl_trainer.py:822] INFO Epoch 49, lr: 0.100000, val loss: 2.302611, val top-1 acc: 10.013752, top-5 acc: 50.011958
2020-03-25 13:23:20,201 [dl_trainer.py:732] WARNING [ 49][ 1240/   25][rank:0] loss: 2.302, average forward (0.004284) and backward (0.455596) time: 0.851947, iotime: 0.248553 
2020-03-25 13:23:26,603 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916276, Speed: 1117.566932 images/s
2020-03-25 13:23:26,603 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:23:26,604 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:23:28,320 [dl_trainer.py:634] INFO train iter: 1250, num_batches_per_epoch: 25
2020-03-25 13:23:28,320 [dl_trainer.py:635] INFO Epoch 50, avg train acc: 9.816406, lr: 0.100000, avg loss: 2.366895
2020-03-25 13:23:31,244 [dl_trainer.py:822] INFO Epoch 50, lr: 0.100000, val loss: 2.302592, val top-1 acc: 10.013752, top-5 acc: 50.083705
2020-03-25 13:23:48,639 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.918122, Speed: 1115.320279 images/s
2020-03-25 13:23:48,640 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:23:48,640 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:23:51,128 [dl_trainer.py:634] INFO train iter: 1275, num_batches_per_epoch: 25
2020-03-25 13:23:51,129 [dl_trainer.py:635] INFO Epoch 51, avg train acc: 9.914062, lr: 0.100000, avg loss: 2.674324
2020-03-25 13:23:53,991 [dl_trainer.py:822] INFO Epoch 51, lr: 0.100000, val loss: 2.302596, val top-1 acc: 10.010762, top-5 acc: 50.038863
2020-03-25 13:23:57,820 [dl_trainer.py:732] WARNING [ 51][ 1280/   25][rank:0] loss: 2.303, average forward (0.004273) and backward (0.451375) time: 0.847550, iotime: 0.246528 
2020-03-25 13:24:10,581 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914177, Speed: 1120.133032 images/s
2020-03-25 13:24:10,582 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:24:10,582 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:24:13,894 [dl_trainer.py:634] INFO train iter: 1300, num_batches_per_epoch: 25
2020-03-25 13:24:13,894 [dl_trainer.py:635] INFO Epoch 52, avg train acc: 9.746094, lr: 0.100000, avg loss: 2.699463
2020-03-25 13:24:16,747 [dl_trainer.py:822] INFO Epoch 52, lr: 0.100000, val loss: 2.302604, val top-1 acc: 10.004783, top-5 acc: 50.005979
2020-03-25 13:24:32,467 [dl_trainer.py:732] WARNING [ 52][ 1320/   25][rank:0] loss: 2.303, average forward (0.004266) and backward (0.454107) time: 0.767367, iotime: 0.237071 
2020-03-25 13:24:32,578 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916491, Speed: 1117.304987 images/s
2020-03-25 13:24:32,578 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:24:32,578 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:24:36,681 [dl_trainer.py:634] INFO train iter: 1325, num_batches_per_epoch: 25
2020-03-25 13:24:36,682 [dl_trainer.py:635] INFO Epoch 53, avg train acc: 9.660156, lr: 0.100000, avg loss: 2.411386
2020-03-25 13:24:39,532 [dl_trainer.py:822] INFO Epoch 53, lr: 0.100000, val loss: 2.302623, val top-1 acc: 10.010762, top-5 acc: 49.988042
2020-03-25 13:24:54,525 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914446, Speed: 1119.803567 images/s
2020-03-25 13:24:54,526 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:24:54,526 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:24:59,407 [dl_trainer.py:634] INFO train iter: 1350, num_batches_per_epoch: 25
2020-03-25 13:24:59,407 [dl_trainer.py:635] INFO Epoch 54, avg train acc: 9.984375, lr: 0.100000, avg loss: 2.622805
2020-03-25 13:25:02,261 [dl_trainer.py:822] INFO Epoch 54, lr: 0.100000, val loss: 2.302620, val top-1 acc: 10.025710, top-5 acc: 50.083705
2020-03-25 13:25:10,038 [dl_trainer.py:732] WARNING [ 54][ 1360/   25][rank:0] loss: 2.303, average forward (0.004237) and backward (0.452118) time: 0.839491, iotime: 0.239905 
2020-03-25 13:25:16,516 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916256, Speed: 1117.591953 images/s
2020-03-25 13:25:16,517 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:25:16,517 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:25:22,218 [dl_trainer.py:634] INFO train iter: 1375, num_batches_per_epoch: 25
2020-03-25 13:25:22,218 [dl_trainer.py:635] INFO Epoch 55, avg train acc: 10.144531, lr: 0.100000, avg loss: 2.414296
2020-03-25 13:25:25,083 [dl_trainer.py:822] INFO Epoch 55, lr: 0.100000, val loss: 2.302629, val top-1 acc: 9.968909, top-5 acc: 50.017937
2020-03-25 13:25:38,499 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915899, Speed: 1118.027341 images/s
2020-03-25 13:25:38,499 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:25:38,499 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:25:44,867 [dl_trainer.py:732] WARNING [ 55][ 1400/   25][rank:0] loss: 2.302, average forward (0.004236) and backward (0.455056) time: 0.771835, iotime: 0.240578 
2020-03-25 13:25:44,961 [dl_trainer.py:634] INFO train iter: 1400, num_batches_per_epoch: 25
2020-03-25 13:25:44,961 [dl_trainer.py:635] INFO Epoch 56, avg train acc: 9.859375, lr: 0.100000, avg loss: 2.697994
2020-03-25 13:25:47,807 [dl_trainer.py:822] INFO Epoch 56, lr: 0.100000, val loss: 2.302605, val top-1 acc: 9.968909, top-5 acc: 49.982063
2020-03-25 13:26:00,411 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912971, Speed: 1121.612661 images/s
2020-03-25 13:26:00,412 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:26:00,412 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:26:07,615 [dl_trainer.py:634] INFO train iter: 1425, num_batches_per_epoch: 25
2020-03-25 13:26:07,615 [dl_trainer.py:635] INFO Epoch 57, avg train acc: 10.304688, lr: 0.100000, avg loss: 2.302594
2020-03-25 13:26:10,479 [dl_trainer.py:822] INFO Epoch 57, lr: 0.100000, val loss: 2.302669, val top-1 acc: 9.983857, top-5 acc: 49.976084
2020-03-25 13:26:22,148 [dl_trainer.py:732] WARNING [ 57][ 1440/   25][rank:0] loss: 2.302, average forward (0.004226) and backward (0.449667) time: 0.831497, iotime: 0.234261 
2020-03-25 13:26:22,291 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911611, Speed: 1123.286101 images/s
2020-03-25 13:26:22,291 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:26:22,291 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:26:30,338 [dl_trainer.py:634] INFO train iter: 1450, num_batches_per_epoch: 25
2020-03-25 13:26:30,338 [dl_trainer.py:635] INFO Epoch 58, avg train acc: 9.687500, lr: 0.100000, avg loss: 2.302817
2020-03-25 13:26:33,159 [dl_trainer.py:822] INFO Epoch 58, lr: 0.100000, val loss: 2.302627, val top-1 acc: 9.998804, top-5 acc: 49.973095
2020-03-25 13:26:44,184 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912205, Speed: 1122.555030 images/s
2020-03-25 13:26:44,185 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:26:44,185 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:26:52,979 [dl_trainer.py:634] INFO train iter: 1475, num_batches_per_epoch: 25
2020-03-25 13:26:52,979 [dl_trainer.py:635] INFO Epoch 59, avg train acc: 9.875000, lr: 0.100000, avg loss: 2.302691
2020-03-25 13:26:55,824 [dl_trainer.py:822] INFO Epoch 59, lr: 0.100000, val loss: 2.302646, val top-1 acc: 10.010762, top-5 acc: 50.032884
2020-03-25 13:26:59,625 [dl_trainer.py:732] WARNING [ 59][ 1480/   25][rank:0] loss: 2.303, average forward (0.004243) and backward (0.449129) time: 0.829790, iotime: 0.234187 
2020-03-25 13:27:06,085 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912463, Speed: 1122.236712 images/s
2020-03-25 13:27:06,085 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:27:06,085 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:27:15,688 [dl_trainer.py:634] INFO train iter: 1500, num_batches_per_epoch: 25
2020-03-25 13:27:15,688 [dl_trainer.py:635] INFO Epoch 60, avg train acc: 9.695312, lr: 0.100000, avg loss: 2.302712
2020-03-25 13:27:18,510 [dl_trainer.py:822] INFO Epoch 60, lr: 0.100000, val loss: 2.302600, val top-1 acc: 10.025710, top-5 acc: 49.982063
2020-03-25 13:27:27,983 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912389, Speed: 1122.328464 images/s
2020-03-25 13:27:27,984 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:27:27,984 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:27:34,302 [dl_trainer.py:732] WARNING [ 60][ 1520/   25][rank:0] loss: 2.302, average forward (0.004233) and backward (0.449151) time: 0.764588, iotime: 0.240084 
2020-03-25 13:27:38,376 [dl_trainer.py:634] INFO train iter: 1525, num_batches_per_epoch: 25
2020-03-25 13:27:38,377 [dl_trainer.py:635] INFO Epoch 61, avg train acc: 9.515625, lr: 0.100000, avg loss: 2.302716
2020-03-25 13:27:41,198 [dl_trainer.py:822] INFO Epoch 61, lr: 0.100000, val loss: 2.302607, val top-1 acc: 10.034678, top-5 acc: 50.023916
2020-03-25 13:27:49,830 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.910219, Speed: 1125.003487 images/s
2020-03-25 13:27:49,830 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:27:49,831 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:28:01,025 [dl_trainer.py:634] INFO train iter: 1550, num_batches_per_epoch: 25
2020-03-25 13:28:01,026 [dl_trainer.py:635] INFO Epoch 62, avg train acc: 9.769531, lr: 0.100000, avg loss: 2.302675
2020-03-25 13:28:03,885 [dl_trainer.py:822] INFO Epoch 62, lr: 0.100000, val loss: 2.302637, val top-1 acc: 10.010762, top-5 acc: 50.017937
2020-03-25 13:28:11,638 [dl_trainer.py:732] WARNING [ 62][ 1560/   25][rank:0] loss: 2.303, average forward (0.004261) and backward (0.449280) time: 0.833187, iotime: 0.236936 
2020-03-25 13:28:11,721 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912113, Speed: 1122.667327 images/s
2020-03-25 13:28:11,722 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:28:11,722 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:28:23,744 [dl_trainer.py:634] INFO train iter: 1575, num_batches_per_epoch: 25
2020-03-25 13:28:23,745 [dl_trainer.py:635] INFO Epoch 63, avg train acc: 10.070312, lr: 0.100000, avg loss: 2.302663
2020-03-25 13:28:26,593 [dl_trainer.py:822] INFO Epoch 63, lr: 0.100000, val loss: 2.302642, val top-1 acc: 10.004783, top-5 acc: 50.005979
2020-03-25 13:28:33,666 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914310, Speed: 1119.969949 images/s
2020-03-25 13:28:33,666 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:28:33,666 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:28:46,347 [dl_trainer.py:732] WARNING [ 63][ 1600/   25][rank:0] loss: 2.303, average forward (0.004237) and backward (0.450298) time: 0.763247, iotime: 0.237172 
2020-03-25 13:28:46,442 [dl_trainer.py:634] INFO train iter: 1600, num_batches_per_epoch: 25
2020-03-25 13:28:46,443 [dl_trainer.py:635] INFO Epoch 64, avg train acc: 9.687500, lr: 0.100000, avg loss: 2.475163
2020-03-25 13:28:49,289 [dl_trainer.py:822] INFO Epoch 64, lr: 0.100000, val loss: 2.302648, val top-1 acc: 9.968909, top-5 acc: 49.934232
2020-03-25 13:28:55,574 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912821, Speed: 1121.797234 images/s
2020-03-25 13:28:55,575 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:28:55,575 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:29:09,219 [dl_trainer.py:634] INFO train iter: 1625, num_batches_per_epoch: 25
2020-03-25 13:29:09,220 [dl_trainer.py:635] INFO Epoch 65, avg train acc: 9.734375, lr: 0.100000, avg loss: 2.302684
2020-03-25 13:29:12,067 [dl_trainer.py:822] INFO Epoch 65, lr: 0.100000, val loss: 2.302648, val top-1 acc: 9.989836, top-5 acc: 50.038863
2020-03-25 13:29:17,546 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915452, Speed: 1118.573662 images/s
2020-03-25 13:29:17,547 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:29:17,547 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:29:23,936 [dl_trainer.py:732] WARNING [ 65][ 1640/   25][rank:0] loss: 2.303, average forward (0.004228) and backward (0.449865) time: 0.840396, iotime: 0.243301 
2020-03-25 13:29:31,955 [dl_trainer.py:634] INFO train iter: 1650, num_batches_per_epoch: 25
2020-03-25 13:29:31,955 [dl_trainer.py:635] INFO Epoch 66, avg train acc: 9.585938, lr: 0.100000, avg loss: 2.302703
2020-03-25 13:29:34,804 [dl_trainer.py:822] INFO Epoch 66, lr: 0.100000, val loss: 2.302624, val top-1 acc: 10.034678, top-5 acc: 50.002990
2020-03-25 13:29:39,497 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914562, Speed: 1119.661376 images/s
2020-03-25 13:29:39,497 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:29:39,497 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:29:54,710 [dl_trainer.py:634] INFO train iter: 1675, num_batches_per_epoch: 25
2020-03-25 13:29:54,710 [dl_trainer.py:635] INFO Epoch 67, avg train acc: 9.933594, lr: 0.100000, avg loss: 2.302704
2020-03-25 13:29:57,557 [dl_trainer.py:822] INFO Epoch 67, lr: 0.100000, val loss: 2.302622, val top-1 acc: 9.998804, top-5 acc: 49.988042
2020-03-25 13:30:01,350 [dl_trainer.py:732] WARNING [ 67][ 1680/   25][rank:0] loss: 2.303, average forward (0.004241) and backward (0.449134) time: 0.838134, iotime: 0.241614 
2020-03-25 13:30:01,450 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914699, Speed: 1119.494272 images/s
2020-03-25 13:30:01,451 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:30:01,451 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:30:17,394 [dl_trainer.py:634] INFO train iter: 1700, num_batches_per_epoch: 25
2020-03-25 13:30:17,395 [dl_trainer.py:635] INFO Epoch 68, avg train acc: 9.984375, lr: 0.100000, avg loss: 2.302567
2020-03-25 13:30:20,243 [dl_trainer.py:822] INFO Epoch 68, lr: 0.100000, val loss: 2.302630, val top-1 acc: 9.989836, top-5 acc: 50.017937
2020-03-25 13:30:23,423 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915500, Speed: 1118.514879 images/s
2020-03-25 13:30:23,424 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:30:23,424 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:30:36,112 [dl_trainer.py:732] WARNING [ 68][ 1720/   25][rank:0] loss: 2.303, average forward (0.004220) and backward (0.449079) time: 0.771198, iotime: 0.246023 
2020-03-25 13:30:40,158 [dl_trainer.py:634] INFO train iter: 1725, num_batches_per_epoch: 25
2020-03-25 13:30:40,158 [dl_trainer.py:635] INFO Epoch 69, avg train acc: 9.671875, lr: 0.100000, avg loss: 2.302651
2020-03-25 13:30:43,001 [dl_trainer.py:822] INFO Epoch 69, lr: 0.100000, val loss: 2.302628, val top-1 acc: 9.968909, top-5 acc: 49.988042
2020-03-25 13:30:45,368 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914336, Speed: 1119.938700 images/s
2020-03-25 13:30:45,369 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:30:45,369 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:31:02,887 [dl_trainer.py:634] INFO train iter: 1750, num_batches_per_epoch: 25
2020-03-25 13:31:02,888 [dl_trainer.py:635] INFO Epoch 70, avg train acc: 9.859375, lr: 0.100000, avg loss: 2.302697
2020-03-25 13:31:05,729 [dl_trainer.py:822] INFO Epoch 70, lr: 0.100000, val loss: 2.302615, val top-1 acc: 9.968909, top-5 acc: 50.047832
2020-03-25 13:31:07,258 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912060, Speed: 1122.733737 images/s
2020-03-25 13:31:07,259 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:31:07,259 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:31:13,611 [dl_trainer.py:732] WARNING [ 70][ 1760/   25][rank:0] loss: 2.302, average forward (0.004222) and backward (0.448921) time: 0.840551, iotime: 0.244661 
2020-03-25 13:31:25,609 [dl_trainer.py:634] INFO train iter: 1775, num_batches_per_epoch: 25
2020-03-25 13:31:25,609 [dl_trainer.py:635] INFO Epoch 71, avg train acc: 10.031250, lr: 0.100000, avg loss: 2.302622
2020-03-25 13:31:28,448 [dl_trainer.py:822] INFO Epoch 71, lr: 0.100000, val loss: 2.302607, val top-1 acc: 9.989836, top-5 acc: 50.000000
2020-03-25 13:31:29,170 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912962, Speed: 1121.624414 images/s
2020-03-25 13:31:29,171 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:31:29,171 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:31:48,177 [dl_trainer.py:732] WARNING [ 71][ 1800/   25][rank:0] loss: 2.303, average forward (0.004238) and backward (0.450212) time: 0.766266, iotime: 0.240488 
2020-03-25 13:31:48,270 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.795786, Speed: 1286.778163 images/s
2020-03-25 13:31:48,270 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:31:48,271 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:31:48,271 [dl_trainer.py:634] INFO train iter: 1800, num_batches_per_epoch: 25
2020-03-25 13:31:48,271 [dl_trainer.py:635] INFO Epoch 72, avg train acc: 9.832031, lr: 0.100000, avg loss: 2.435798
2020-03-25 13:31:51,121 [dl_trainer.py:822] INFO Epoch 72, lr: 0.100000, val loss: 2.302608, val top-1 acc: 10.004783, top-5 acc: 50.011958
2020-03-25 13:32:10,153 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911770, Speed: 1123.090637 images/s
2020-03-25 13:32:10,154 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:32:10,154 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:32:11,067 [dl_trainer.py:634] INFO train iter: 1825, num_batches_per_epoch: 25
2020-03-25 13:32:11,068 [dl_trainer.py:635] INFO Epoch 73, avg train acc: 9.816406, lr: 0.100000, avg loss: 2.302674
2020-03-25 13:32:13,915 [dl_trainer.py:822] INFO Epoch 73, lr: 0.100000, val loss: 2.302606, val top-1 acc: 9.983857, top-5 acc: 49.934232
2020-03-25 13:32:25,710 [dl_trainer.py:732] WARNING [ 73][ 1840/   25][rank:0] loss: 2.303, average forward (0.004246) and backward (0.449135) time: 0.840259, iotime: 0.243748 
2020-03-25 13:32:32,148 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916388, Speed: 1117.430384 images/s
2020-03-25 13:32:32,148 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:32:32,148 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:32:33,842 [dl_trainer.py:634] INFO train iter: 1850, num_batches_per_epoch: 25
2020-03-25 13:32:33,843 [dl_trainer.py:635] INFO Epoch 74, avg train acc: 9.945312, lr: 0.100000, avg loss: 2.302580
2020-03-25 13:32:36,687 [dl_trainer.py:822] INFO Epoch 74, lr: 0.100000, val loss: 2.302597, val top-1 acc: 10.025710, top-5 acc: 50.017937
2020-03-25 13:32:54,045 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912353, Speed: 1122.373032 images/s
2020-03-25 13:32:54,046 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:32:54,046 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:32:56,535 [dl_trainer.py:634] INFO train iter: 1875, num_batches_per_epoch: 25
2020-03-25 13:32:56,536 [dl_trainer.py:635] INFO Epoch 75, avg train acc: 9.921875, lr: 0.100000, avg loss: 2.302725
2020-03-25 13:32:59,382 [dl_trainer.py:822] INFO Epoch 75, lr: 0.100000, val loss: 2.302592, val top-1 acc: 10.004783, top-5 acc: 50.023916
2020-03-25 13:33:03,180 [dl_trainer.py:732] WARNING [ 75][ 1880/   25][rank:0] loss: 2.303, average forward (0.004241) and backward (0.449116) time: 0.840700, iotime: 0.244403 
2020-03-25 13:33:15,937 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912140, Speed: 1122.635048 images/s
2020-03-25 13:33:15,938 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:33:15,938 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:33:19,236 [dl_trainer.py:634] INFO train iter: 1900, num_batches_per_epoch: 25
2020-03-25 13:33:19,237 [dl_trainer.py:635] INFO Epoch 76, avg train acc: 10.011719, lr: 0.100000, avg loss: 2.302719
2020-03-25 13:33:22,088 [dl_trainer.py:822] INFO Epoch 76, lr: 0.100000, val loss: 2.302614, val top-1 acc: 9.968909, top-5 acc: 49.952168
2020-03-25 13:33:37,812 [dl_trainer.py:732] WARNING [ 76][ 1920/   25][rank:0] loss: 2.303, average forward (0.004252) and backward (0.449223) time: 0.770084, iotime: 0.244607 
2020-03-25 13:33:37,914 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915653, Speed: 1118.327831 images/s
2020-03-25 13:33:37,914 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:33:37,915 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:33:41,985 [dl_trainer.py:634] INFO train iter: 1925, num_batches_per_epoch: 25
2020-03-25 13:33:41,986 [dl_trainer.py:635] INFO Epoch 77, avg train acc: 9.656250, lr: 0.100000, avg loss: 2.302727
2020-03-25 13:33:44,835 [dl_trainer.py:822] INFO Epoch 77, lr: 0.100000, val loss: 2.302599, val top-1 acc: 10.034678, top-5 acc: 50.059790
2020-03-25 13:33:59,847 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.913822, Speed: 1120.568527 images/s
2020-03-25 13:33:59,847 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:33:59,847 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:34:04,719 [dl_trainer.py:634] INFO train iter: 1950, num_batches_per_epoch: 25
2020-03-25 13:34:04,719 [dl_trainer.py:635] INFO Epoch 78, avg train acc: 9.937500, lr: 0.100000, avg loss: 2.302666
2020-03-25 13:34:07,578 [dl_trainer.py:822] INFO Epoch 78, lr: 0.100000, val loss: 2.302609, val top-1 acc: 10.004783, top-5 acc: 49.946189
2020-03-25 13:34:15,370 [dl_trainer.py:732] WARNING [ 78][ 1960/   25][rank:0] loss: 2.302, average forward (0.004241) and backward (0.449200) time: 0.839342, iotime: 0.242487 
2020-03-25 13:34:21,835 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916137, Speed: 1117.736309 images/s
2020-03-25 13:34:21,836 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:34:21,836 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:34:27,508 [dl_trainer.py:634] INFO train iter: 1975, num_batches_per_epoch: 25
2020-03-25 13:34:27,508 [dl_trainer.py:635] INFO Epoch 79, avg train acc: 9.953125, lr: 0.100000, avg loss: 2.302658
2020-03-25 13:34:30,354 [dl_trainer.py:822] INFO Epoch 79, lr: 0.100000, val loss: 2.302596, val top-1 acc: 10.013752, top-5 acc: 49.976084
2020-03-25 13:34:43,776 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914162, Speed: 1120.151376 images/s
2020-03-25 13:34:43,776 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:34:43,777 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:34:50,170 [dl_trainer.py:732] WARNING [ 79][ 2000/   25][rank:0] loss: 2.303, average forward (0.004232) and backward (0.449207) time: 0.768556, iotime: 0.243520 
2020-03-25 13:34:50,268 [dl_trainer.py:634] INFO train iter: 2000, num_batches_per_epoch: 25
2020-03-25 13:34:50,268 [dl_trainer.py:635] INFO Epoch 80, avg train acc: 9.625000, lr: 0.100000, avg loss: 2.302712
2020-03-25 13:34:53,146 [dl_trainer.py:822] INFO Epoch 80, lr: 0.100000, val loss: 2.302609, val top-1 acc: 10.013752, top-5 acc: 50.005979
2020-03-25 13:35:05,803 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.917748, Speed: 1115.774949 images/s
2020-03-25 13:35:05,804 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:35:05,804 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:35:13,062 [dl_trainer.py:634] INFO train iter: 2025, num_batches_per_epoch: 25
2020-03-25 13:35:13,063 [dl_trainer.py:635] INFO Epoch 81, avg train acc: 10.031250, lr: 0.100000, avg loss: 2.302777
2020-03-25 13:35:15,917 [dl_trainer.py:822] INFO Epoch 81, lr: 0.100000, val loss: 2.302636, val top-1 acc: 9.983857, top-5 acc: 50.008968
2020-03-25 13:35:27,658 [dl_trainer.py:732] WARNING [ 81][ 2040/   25][rank:0] loss: 2.303, average forward (0.004233) and backward (0.449230) time: 0.838674, iotime: 0.241179 
2020-03-25 13:35:27,758 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914750, Speed: 1119.432062 images/s
2020-03-25 13:35:27,758 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:35:27,758 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:35:35,816 [dl_trainer.py:634] INFO train iter: 2050, num_batches_per_epoch: 25
2020-03-25 13:35:35,816 [dl_trainer.py:635] INFO Epoch 82, avg train acc: 9.812500, lr: 0.010000, avg loss: 2.302783
2020-03-25 13:35:38,661 [dl_trainer.py:822] INFO Epoch 82, lr: 0.010000, val loss: 2.302696, val top-1 acc: 9.983857, top-5 acc: 49.979074
2020-03-25 13:35:49,719 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915017, Speed: 1119.105135 images/s
2020-03-25 13:35:49,720 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:35:49,720 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:35:58,566 [dl_trainer.py:634] INFO train iter: 2075, num_batches_per_epoch: 25
2020-03-25 13:35:58,566 [dl_trainer.py:635] INFO Epoch 83, avg train acc: 9.765625, lr: 0.010000, avg loss: 2.302769
2020-03-25 13:36:01,416 [dl_trainer.py:822] INFO Epoch 83, lr: 0.010000, val loss: 2.302657, val top-1 acc: 9.983857, top-5 acc: 49.979074
2020-03-25 13:36:05,220 [dl_trainer.py:732] WARNING [ 83][ 2080/   25][rank:0] loss: 2.302, average forward (0.004253) and backward (0.449077) time: 0.840137, iotime: 0.243722 
2020-03-25 13:36:11,673 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914717, Speed: 1119.472497 images/s
2020-03-25 13:36:11,674 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:36:11,674 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:36:21,344 [dl_trainer.py:634] INFO train iter: 2100, num_batches_per_epoch: 25
2020-03-25 13:36:21,345 [dl_trainer.py:635] INFO Epoch 84, avg train acc: 9.984375, lr: 0.010000, avg loss: 2.392528
2020-03-25 13:36:24,189 [dl_trainer.py:822] INFO Epoch 84, lr: 0.010000, val loss: 2.302637, val top-1 acc: 9.983857, top-5 acc: 50.008968
2020-03-25 13:36:33,663 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916206, Speed: 1117.652069 images/s
2020-03-25 13:36:33,664 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:36:33,664 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:36:40,026 [dl_trainer.py:732] WARNING [ 84][ 2120/   25][rank:0] loss: 2.302, average forward (0.004258) and backward (0.449746) time: 0.770481, iotime: 0.244707 
2020-03-25 13:36:44,092 [dl_trainer.py:634] INFO train iter: 2125, num_batches_per_epoch: 25
2020-03-25 13:36:44,093 [dl_trainer.py:635] INFO Epoch 85, avg train acc: 9.937500, lr: 0.010000, avg loss: 2.302655
2020-03-25 13:36:46,941 [dl_trainer.py:822] INFO Epoch 85, lr: 0.010000, val loss: 2.302619, val top-1 acc: 9.983857, top-5 acc: 49.979074
2020-03-25 13:36:55,608 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914327, Speed: 1119.949396 images/s
2020-03-25 13:36:55,609 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:36:55,609 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:37:06,839 [dl_trainer.py:634] INFO train iter: 2150, num_batches_per_epoch: 25
2020-03-25 13:37:06,839 [dl_trainer.py:635] INFO Epoch 86, avg train acc: 9.746094, lr: 0.010000, avg loss: 2.302651
2020-03-25 13:37:09,683 [dl_trainer.py:822] INFO Epoch 86, lr: 0.010000, val loss: 2.302604, val top-1 acc: 9.983857, top-5 acc: 49.979074
2020-03-25 13:37:17,482 [dl_trainer.py:732] WARNING [ 86][ 2160/   25][rank:0] loss: 2.303, average forward (0.004223) and backward (0.449051) time: 0.838584, iotime: 0.242212 
2020-03-25 13:37:17,586 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915714, Speed: 1118.253606 images/s
2020-03-25 13:37:17,587 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:37:17,587 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:37:29,592 [dl_trainer.py:634] INFO train iter: 2175, num_batches_per_epoch: 25
2020-03-25 13:37:29,593 [dl_trainer.py:635] INFO Epoch 87, avg train acc: 10.109375, lr: 0.010000, avg loss: 2.302590
2020-03-25 13:37:32,430 [dl_trainer.py:822] INFO Epoch 87, lr: 0.010000, val loss: 2.302596, val top-1 acc: 9.989836, top-5 acc: 49.979074
2020-03-25 13:37:39,494 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912789, Speed: 1121.836644 images/s
2020-03-25 13:37:39,495 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:37:39,495 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:37:52,205 [dl_trainer.py:732] WARNING [ 87][ 2200/   25][rank:0] loss: 2.302, average forward (0.004243) and backward (0.448860) time: 0.769627, iotime: 0.245127 
2020-03-25 13:37:52,291 [dl_trainer.py:634] INFO train iter: 2200, num_batches_per_epoch: 25
2020-03-25 13:37:52,292 [dl_trainer.py:635] INFO Epoch 88, avg train acc: 9.910156, lr: 0.010000, avg loss: 2.302576
2020-03-25 13:37:55,163 [dl_trainer.py:822] INFO Epoch 88, lr: 0.010000, val loss: 2.302594, val top-1 acc: 9.983857, top-5 acc: 49.979074
2020-03-25 13:38:01,523 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.917836, Speed: 1115.667819 images/s
2020-03-25 13:38:01,524 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:38:01,524 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:38:15,155 [dl_trainer.py:634] INFO train iter: 2225, num_batches_per_epoch: 25
2020-03-25 13:38:15,155 [dl_trainer.py:635] INFO Epoch 89, avg train acc: 10.078125, lr: 0.010000, avg loss: 2.302602
2020-03-25 13:38:18,021 [dl_trainer.py:822] INFO Epoch 89, lr: 0.010000, val loss: 2.302591, val top-1 acc: 9.983857, top-5 acc: 50.008968
2020-03-25 13:38:23,494 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915399, Speed: 1118.637550 images/s
2020-03-25 13:38:23,494 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:38:23,494 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:38:29,853 [dl_trainer.py:732] WARNING [ 89][ 2240/   25][rank:0] loss: 2.303, average forward (0.004253) and backward (0.448901) time: 0.838980, iotime: 0.241692 
2020-03-25 13:38:37,874 [dl_trainer.py:634] INFO train iter: 2250, num_batches_per_epoch: 25
2020-03-25 13:38:37,874 [dl_trainer.py:635] INFO Epoch 90, avg train acc: 9.945312, lr: 0.010000, avg loss: 2.302602
2020-03-25 13:38:40,727 [dl_trainer.py:822] INFO Epoch 90, lr: 0.010000, val loss: 2.302588, val top-1 acc: 9.983857, top-5 acc: 50.032884
2020-03-25 13:38:45,480 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916056, Speed: 1117.835873 images/s
2020-03-25 13:38:45,480 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:38:45,481 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:39:00,657 [dl_trainer.py:634] INFO train iter: 2275, num_batches_per_epoch: 25
2020-03-25 13:39:00,657 [dl_trainer.py:635] INFO Epoch 91, avg train acc: 9.664062, lr: 0.010000, avg loss: 2.302615
2020-03-25 13:39:03,532 [dl_trainer.py:822] INFO Epoch 91, lr: 0.010000, val loss: 2.302586, val top-1 acc: 9.983857, top-5 acc: 50.017937
2020-03-25 13:39:07,330 [dl_trainer.py:732] WARNING [ 91][ 2280/   25][rank:0] loss: 2.303, average forward (0.004235) and backward (0.448873) time: 0.839656, iotime: 0.242497 
2020-03-25 13:39:07,423 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914273, Speed: 1120.015388 images/s
2020-03-25 13:39:07,424 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:39:07,424 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:39:23,494 [dl_trainer.py:634] INFO train iter: 2300, num_batches_per_epoch: 25
2020-03-25 13:39:23,495 [dl_trainer.py:635] INFO Epoch 92, avg train acc: 9.843750, lr: 0.010000, avg loss: 2.302603
2020-03-25 13:39:26,347 [dl_trainer.py:822] INFO Epoch 92, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.989836, top-5 acc: 49.946189
2020-03-25 13:39:29,471 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.918630, Speed: 1114.703786 images/s
2020-03-25 13:39:29,472 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:39:29,472 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:39:42,201 [dl_trainer.py:732] WARNING [ 92][ 2320/   25][rank:0] loss: 2.303, average forward (0.004253) and backward (0.448856) time: 0.770801, iotime: 0.245747 
2020-03-25 13:39:46,250 [dl_trainer.py:634] INFO train iter: 2325, num_batches_per_epoch: 25
2020-03-25 13:39:46,250 [dl_trainer.py:635] INFO Epoch 93, avg train acc: 9.898438, lr: 0.010000, avg loss: 2.302609
2020-03-25 13:39:49,107 [dl_trainer.py:822] INFO Epoch 93, lr: 0.010000, val loss: 2.302588, val top-1 acc: 9.989836, top-5 acc: 49.952168
2020-03-25 13:39:51,472 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916635, Speed: 1117.129408 images/s
2020-03-25 13:39:51,472 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:39:51,472 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:40:09,044 [dl_trainer.py:634] INFO train iter: 2350, num_batches_per_epoch: 25
2020-03-25 13:40:09,045 [dl_trainer.py:635] INFO Epoch 94, avg train acc: 9.796875, lr: 0.010000, avg loss: 2.302600
2020-03-25 13:40:11,894 [dl_trainer.py:822] INFO Epoch 94, lr: 0.010000, val loss: 2.302588, val top-1 acc: 9.989836, top-5 acc: 49.979074
2020-03-25 13:40:13,485 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.917208, Speed: 1116.431925 images/s
2020-03-25 13:40:13,486 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:40:13,486 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:40:19,855 [dl_trainer.py:732] WARNING [ 94][ 2360/   25][rank:0] loss: 2.303, average forward (0.004259) and backward (0.449082) time: 0.844217, iotime: 0.247434 
2020-03-25 13:40:31,834 [dl_trainer.py:634] INFO train iter: 2375, num_batches_per_epoch: 25
2020-03-25 13:40:31,835 [dl_trainer.py:635] INFO Epoch 95, avg train acc: 10.058594, lr: 0.010000, avg loss: 2.302589
2020-03-25 13:40:34,680 [dl_trainer.py:822] INFO Epoch 95, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.968909, top-5 acc: 49.958147
2020-03-25 13:40:35,404 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.913233, Speed: 1121.291376 images/s
2020-03-25 13:40:35,404 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:40:35,404 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:40:54,459 [dl_trainer.py:732] WARNING [ 95][ 2400/   25][rank:0] loss: 2.303, average forward (0.004240) and backward (0.448774) time: 0.764503, iotime: 0.239933 
2020-03-25 13:40:54,552 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.797807, Speed: 1283.518672 images/s
2020-03-25 13:40:54,552 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:40:54,553 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:40:54,553 [dl_trainer.py:634] INFO train iter: 2400, num_batches_per_epoch: 25
2020-03-25 13:40:54,553 [dl_trainer.py:635] INFO Epoch 96, avg train acc: 10.117188, lr: 0.010000, avg loss: 2.302578
2020-03-25 13:40:57,404 [dl_trainer.py:822] INFO Epoch 96, lr: 0.010000, val loss: 2.302586, val top-1 acc: 10.010762, top-5 acc: 49.973095
2020-03-25 13:41:16,477 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.913498, Speed: 1120.965301 images/s
2020-03-25 13:41:16,478 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:41:16,478 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:41:17,390 [dl_trainer.py:634] INFO train iter: 2425, num_batches_per_epoch: 25
2020-03-25 13:41:17,391 [dl_trainer.py:635] INFO Epoch 97, avg train acc: 10.058594, lr: 0.010000, avg loss: 2.302588
2020-03-25 13:41:20,238 [dl_trainer.py:822] INFO Epoch 97, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.989836, top-5 acc: 49.994021
2020-03-25 13:41:31,958 [dl_trainer.py:732] WARNING [ 97][ 2440/   25][rank:0] loss: 2.303, average forward (0.004233) and backward (0.448844) time: 0.840085, iotime: 0.243899 
2020-03-25 13:41:38,393 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.913137, Speed: 1121.408825 images/s
2020-03-25 13:41:38,394 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:41:38,394 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:41:40,092 [dl_trainer.py:634] INFO train iter: 2450, num_batches_per_epoch: 25
2020-03-25 13:41:40,093 [dl_trainer.py:635] INFO Epoch 98, avg train acc: 9.941406, lr: 0.010000, avg loss: 2.302586
2020-03-25 13:41:42,940 [dl_trainer.py:822] INFO Epoch 98, lr: 0.010000, val loss: 2.302585, val top-1 acc: 9.989836, top-5 acc: 49.994021
2020-03-25 13:42:00,308 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.913050, Speed: 1121.515242 images/s
2020-03-25 13:42:00,309 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:42:00,309 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:42:02,801 [dl_trainer.py:634] INFO train iter: 2475, num_batches_per_epoch: 25
2020-03-25 13:42:02,802 [dl_trainer.py:635] INFO Epoch 99, avg train acc: 9.855469, lr: 0.010000, avg loss: 2.302608
2020-03-25 13:42:05,648 [dl_trainer.py:822] INFO Epoch 99, lr: 0.010000, val loss: 2.302585, val top-1 acc: 10.025710, top-5 acc: 49.967116
2020-03-25 13:42:09,458 [dl_trainer.py:732] WARNING [ 99][ 2480/   25][rank:0] loss: 2.303, average forward (0.004240) and backward (0.448791) time: 0.839633, iotime: 0.243512 
2020-03-25 13:42:22,097 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.907822, Speed: 1127.973843 images/s
2020-03-25 13:42:22,097 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:42:22,097 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:42:25,365 [dl_trainer.py:634] INFO train iter: 2500, num_batches_per_epoch: 25
2020-03-25 13:42:25,365 [dl_trainer.py:635] INFO Epoch 100, avg train acc: 9.921875, lr: 0.010000, avg loss: 2.302599
2020-03-25 13:42:28,231 [dl_trainer.py:822] INFO Epoch 100, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.983857, top-5 acc: 49.952168
2020-03-25 13:42:43,876 [dl_trainer.py:732] WARNING [100][ 2520/   25][rank:0] loss: 2.303, average forward (0.004232) and backward (0.448821) time: 0.767806, iotime: 0.242407 
2020-03-25 13:42:43,964 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911089, Speed: 1123.929687 images/s
2020-03-25 13:42:43,964 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:42:43,964 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:42:48,017 [dl_trainer.py:634] INFO train iter: 2525, num_batches_per_epoch: 25
2020-03-25 13:42:48,018 [dl_trainer.py:635] INFO Epoch 101, avg train acc: 9.882812, lr: 0.010000, avg loss: 2.302592
2020-03-25 13:42:50,868 [dl_trainer.py:822] INFO Epoch 101, lr: 0.010000, val loss: 2.302588, val top-1 acc: 9.968909, top-5 acc: 49.931242
2020-03-25 13:43:05,790 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.909373, Speed: 1126.050469 images/s
2020-03-25 13:43:05,790 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:43:05,790 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:43:10,629 [dl_trainer.py:634] INFO train iter: 2550, num_batches_per_epoch: 25
2020-03-25 13:43:10,629 [dl_trainer.py:635] INFO Epoch 102, avg train acc: 10.027344, lr: 0.010000, avg loss: 2.302604
2020-03-25 13:43:13,477 [dl_trainer.py:822] INFO Epoch 102, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.983857, top-5 acc: 49.925263
2020-03-25 13:43:21,248 [dl_trainer.py:732] WARNING [102][ 2560/   25][rank:0] loss: 2.303, average forward (0.004254) and backward (0.449167) time: 0.842735, iotime: 0.245935 
2020-03-25 13:43:27,637 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.910247, Speed: 1124.968790 images/s
2020-03-25 13:43:27,637 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:43:27,637 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:43:33,264 [dl_trainer.py:634] INFO train iter: 2575, num_batches_per_epoch: 25
2020-03-25 13:43:33,264 [dl_trainer.py:635] INFO Epoch 103, avg train acc: 9.894531, lr: 0.010000, avg loss: 2.302599
2020-03-25 13:43:36,108 [dl_trainer.py:822] INFO Epoch 103, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.983857, top-5 acc: 49.967116
2020-03-25 13:43:49,436 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.908261, Speed: 1127.429463 images/s
2020-03-25 13:43:49,437 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:43:49,437 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:43:55,743 [dl_trainer.py:732] WARNING [103][ 2600/   25][rank:0] loss: 2.302, average forward (0.004242) and backward (0.449261) time: 0.770740, iotime: 0.245676 
2020-03-25 13:43:55,842 [dl_trainer.py:634] INFO train iter: 2600, num_batches_per_epoch: 25
2020-03-25 13:43:55,842 [dl_trainer.py:635] INFO Epoch 104, avg train acc: 10.414062, lr: 0.010000, avg loss: 2.302576
2020-03-25 13:43:58,688 [dl_trainer.py:822] INFO Epoch 104, lr: 0.010000, val loss: 2.302587, val top-1 acc: 10.004783, top-5 acc: 49.952168
2020-03-25 13:44:11,240 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.908441, Speed: 1127.206015 images/s
2020-03-25 13:44:11,240 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:44:11,240 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:44:18,428 [dl_trainer.py:634] INFO train iter: 2625, num_batches_per_epoch: 25
2020-03-25 13:44:18,428 [dl_trainer.py:635] INFO Epoch 105, avg train acc: 10.042969, lr: 0.010000, avg loss: 2.302585
2020-03-25 13:44:21,274 [dl_trainer.py:822] INFO Epoch 105, lr: 0.010000, val loss: 2.302587, val top-1 acc: 10.004783, top-5 acc: 49.982063
2020-03-25 13:44:32,966 [dl_trainer.py:732] WARNING [105][ 2640/   25][rank:0] loss: 2.303, average forward (0.004252) and backward (0.449142) time: 0.840525, iotime: 0.244093 
2020-03-25 13:44:33,042 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.908392, Speed: 1127.266295 images/s
2020-03-25 13:44:33,042 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:44:33,042 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:44:41,058 [dl_trainer.py:634] INFO train iter: 2650, num_batches_per_epoch: 25
2020-03-25 13:44:41,058 [dl_trainer.py:635] INFO Epoch 106, avg train acc: 9.984375, lr: 0.010000, avg loss: 2.302596
2020-03-25 13:44:43,904 [dl_trainer.py:822] INFO Epoch 106, lr: 0.010000, val loss: 2.302588, val top-1 acc: 10.004783, top-5 acc: 49.940210
2020-03-25 13:44:54,879 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.909843, Speed: 1125.469025 images/s
2020-03-25 13:44:54,879 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:44:54,880 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:45:03,651 [dl_trainer.py:634] INFO train iter: 2675, num_batches_per_epoch: 25
2020-03-25 13:45:03,651 [dl_trainer.py:635] INFO Epoch 107, avg train acc: 10.039062, lr: 0.010000, avg loss: 2.302585
2020-03-25 13:45:06,488 [dl_trainer.py:822] INFO Epoch 107, lr: 0.010000, val loss: 2.302588, val top-1 acc: 10.004783, top-5 acc: 49.961137
2020-03-25 13:45:10,270 [dl_trainer.py:732] WARNING [107][ 2680/   25][rank:0] loss: 2.303, average forward (0.004298) and backward (0.449090) time: 0.842126, iotime: 0.245894 
2020-03-25 13:45:16,651 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.907152, Speed: 1128.807475 images/s
2020-03-25 13:45:16,652 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:45:16,652 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:45:26,203 [dl_trainer.py:634] INFO train iter: 2700, num_batches_per_epoch: 25
2020-03-25 13:45:26,203 [dl_trainer.py:635] INFO Epoch 108, avg train acc: 9.890625, lr: 0.010000, avg loss: 2.302602
2020-03-25 13:45:29,063 [dl_trainer.py:822] INFO Epoch 108, lr: 0.010000, val loss: 2.302586, val top-1 acc: 10.004783, top-5 acc: 49.970105
2020-03-25 13:45:38,459 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.908609, Speed: 1126.997184 images/s
2020-03-25 13:45:38,460 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:45:38,460 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:45:44,774 [dl_trainer.py:732] WARNING [108][ 2720/   25][rank:0] loss: 2.303, average forward (0.004248) and backward (0.448787) time: 0.770202, iotime: 0.244963 
2020-03-25 13:45:48,802 [dl_trainer.py:634] INFO train iter: 2725, num_batches_per_epoch: 25
2020-03-25 13:45:48,802 [dl_trainer.py:635] INFO Epoch 109, avg train acc: 10.191406, lr: 0.010000, avg loss: 2.302580
2020-03-25 13:45:51,649 [dl_trainer.py:822] INFO Epoch 109, lr: 0.010000, val loss: 2.302587, val top-1 acc: 10.004783, top-5 acc: 49.976084
2020-03-25 13:46:00,287 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.909468, Speed: 1125.933350 images/s
2020-03-25 13:46:00,288 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:46:00,288 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:46:11,437 [dl_trainer.py:634] INFO train iter: 2750, num_batches_per_epoch: 25
2020-03-25 13:46:11,438 [dl_trainer.py:635] INFO Epoch 110, avg train acc: 9.726562, lr: 0.010000, avg loss: 2.302611
2020-03-25 13:46:14,287 [dl_trainer.py:822] INFO Epoch 110, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.983857, top-5 acc: 49.961137
2020-03-25 13:46:22,068 [dl_trainer.py:732] WARNING [110][ 2760/   25][rank:0] loss: 2.303, average forward (0.004246) and backward (0.448733) time: 0.841899, iotime: 0.245595 
2020-03-25 13:46:22,163 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911460, Speed: 1123.472706 images/s
2020-03-25 13:46:22,164 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:46:22,164 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:46:34,079 [dl_trainer.py:634] INFO train iter: 2775, num_batches_per_epoch: 25
2020-03-25 13:46:34,080 [dl_trainer.py:635] INFO Epoch 111, avg train acc: 10.058594, lr: 0.010000, avg loss: 2.302600
2020-03-25 13:46:36,974 [dl_trainer.py:822] INFO Epoch 111, lr: 0.010000, val loss: 2.302587, val top-1 acc: 9.983857, top-5 acc: 49.946189
2020-03-25 13:46:43,987 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.909277, Speed: 1126.169065 images/s
2020-03-25 13:46:43,987 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:46:43,988 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:46:56,603 [dl_trainer.py:732] WARNING [111][ 2800/   25][rank:0] loss: 2.303, average forward (0.004240) and backward (0.448977) time: 0.770129, iotime: 0.244097 
2020-03-25 13:46:56,694 [dl_trainer.py:634] INFO train iter: 2800, num_batches_per_epoch: 25
2020-03-25 13:46:56,694 [dl_trainer.py:635] INFO Epoch 112, avg train acc: 9.656250, lr: 0.010000, avg loss: 2.302595
2020-03-25 13:46:59,546 [dl_trainer.py:822] INFO Epoch 112, lr: 0.010000, val loss: 2.302586, val top-1 acc: 9.998804, top-5 acc: 49.997010
2020-03-25 13:47:05,876 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911989, Speed: 1122.820006 images/s
2020-03-25 13:47:05,877 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:47:05,877 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:47:19,362 [dl_trainer.py:634] INFO train iter: 2825, num_batches_per_epoch: 25
2020-03-25 13:47:19,363 [dl_trainer.py:635] INFO Epoch 113, avg train acc: 9.964844, lr: 0.010000, avg loss: 2.302589
2020-03-25 13:47:22,210 [dl_trainer.py:822] INFO Epoch 113, lr: 0.010000, val loss: 2.302585, val top-1 acc: 10.004783, top-5 acc: 50.008968
2020-03-25 13:47:27,676 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.908304, Speed: 1127.375344 images/s
2020-03-25 13:47:27,677 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:47:27,677 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:47:34,017 [dl_trainer.py:732] WARNING [113][ 2840/   25][rank:0] loss: 2.303, average forward (0.004257) and backward (0.448696) time: 0.842580, iotime: 0.246244 
2020-03-25 13:47:42,017 [dl_trainer.py:634] INFO train iter: 2850, num_batches_per_epoch: 25
2020-03-25 13:47:42,018 [dl_trainer.py:635] INFO Epoch 114, avg train acc: 9.679688, lr: 0.010000, avg loss: 2.302599
2020-03-25 13:47:44,862 [dl_trainer.py:822] INFO Epoch 114, lr: 0.010000, val loss: 2.302585, val top-1 acc: 10.004783, top-5 acc: 50.044842
2020-03-25 13:47:49,560 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911785, Speed: 1123.071805 images/s
2020-03-25 13:47:49,560 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:47:49,561 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:48:04,722 [dl_trainer.py:634] INFO train iter: 2875, num_batches_per_epoch: 25
2020-03-25 13:48:04,723 [dl_trainer.py:635] INFO Epoch 115, avg train acc: 9.800781, lr: 0.010000, avg loss: 2.302611
2020-03-25 13:48:07,565 [dl_trainer.py:822] INFO Epoch 115, lr: 0.010000, val loss: 2.302585, val top-1 acc: 10.004783, top-5 acc: 50.020926
2020-03-25 13:48:11,359 [dl_trainer.py:732] WARNING [115][ 2880/   25][rank:0] loss: 2.303, average forward (0.004258) and backward (0.448710) time: 0.837593, iotime: 0.241724 
2020-03-25 13:48:11,455 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912244, Speed: 1122.506597 images/s
2020-03-25 13:48:11,455 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:48:11,455 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:48:27,392 [dl_trainer.py:634] INFO train iter: 2900, num_batches_per_epoch: 25
2020-03-25 13:48:27,392 [dl_trainer.py:635] INFO Epoch 116, avg train acc: 10.078125, lr: 0.010000, avg loss: 2.302593
2020-03-25 13:48:30,231 [dl_trainer.py:822] INFO Epoch 116, lr: 0.010000, val loss: 2.302586, val top-1 acc: 10.034678, top-5 acc: 50.023916
2020-03-25 13:48:33,445 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.916242, Speed: 1117.608323 images/s
2020-03-25 13:48:33,446 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:48:33,446 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:48:46,106 [dl_trainer.py:732] WARNING [116][ 2920/   25][rank:0] loss: 2.303, average forward (0.004258) and backward (0.448947) time: 0.771409, iotime: 0.245391 
2020-03-25 13:48:50,148 [dl_trainer.py:634] INFO train iter: 2925, num_batches_per_epoch: 25
2020-03-25 13:48:50,148 [dl_trainer.py:635] INFO Epoch 117, avg train acc: 9.742188, lr: 0.010000, avg loss: 2.302606
2020-03-25 13:48:53,002 [dl_trainer.py:822] INFO Epoch 117, lr: 0.010000, val loss: 2.302584, val top-1 acc: 10.034678, top-5 acc: 50.032884
2020-03-25 13:48:55,316 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911237, Speed: 1123.747083 images/s
2020-03-25 13:48:55,316 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:48:55,316 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:49:12,829 [dl_trainer.py:634] INFO train iter: 2950, num_batches_per_epoch: 25
2020-03-25 13:49:12,830 [dl_trainer.py:635] INFO Epoch 118, avg train acc: 9.683594, lr: 0.010000, avg loss: 2.302594
2020-03-25 13:49:15,658 [dl_trainer.py:822] INFO Epoch 118, lr: 0.010000, val loss: 2.302586, val top-1 acc: 9.968909, top-5 acc: 49.997010
2020-03-25 13:49:17,182 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911056, Speed: 1123.970411 images/s
2020-03-25 13:49:17,182 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:49:17,182 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:49:23,522 [dl_trainer.py:732] WARNING [118][ 2960/   25][rank:0] loss: 2.303, average forward (0.004263) and backward (0.448733) time: 0.838497, iotime: 0.242745 
2020-03-25 13:49:35,478 [dl_trainer.py:634] INFO train iter: 2975, num_batches_per_epoch: 25
2020-03-25 13:49:35,479 [dl_trainer.py:635] INFO Epoch 119, avg train acc: 9.703125, lr: 0.010000, avg loss: 2.302602
2020-03-25 13:49:38,326 [dl_trainer.py:822] INFO Epoch 119, lr: 0.010000, val loss: 2.302586, val top-1 acc: 9.968909, top-5 acc: 50.011958
2020-03-25 13:49:39,048 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911074, Speed: 1123.948253 images/s
2020-03-25 13:49:39,049 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:49:39,049 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:49:58,069 [dl_trainer.py:732] WARNING [119][ 3000/   25][rank:0] loss: 2.303, average forward (0.004256) and backward (0.448604) time: 0.766035, iotime: 0.241621 
2020-03-25 13:49:58,162 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.796380, Speed: 1285.817697 images/s
2020-03-25 13:49:58,163 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:49:58,163 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:49:58,164 [dl_trainer.py:634] INFO train iter: 3000, num_batches_per_epoch: 25
2020-03-25 13:49:58,164 [dl_trainer.py:635] INFO Epoch 120, avg train acc: 9.816406, lr: 0.010000, avg loss: 2.302593
2020-03-25 13:50:01,012 [dl_trainer.py:822] INFO Epoch 120, lr: 0.010000, val loss: 2.302586, val top-1 acc: 10.004783, top-5 acc: 50.023916
2020-03-25 13:50:20,078 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.913098, Speed: 1121.456455 images/s
2020-03-25 13:50:20,078 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:50:20,078 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:50:20,987 [dl_trainer.py:634] INFO train iter: 3025, num_batches_per_epoch: 25
2020-03-25 13:50:20,988 [dl_trainer.py:635] INFO Epoch 121, avg train acc: 9.953125, lr: 0.010000, avg loss: 2.302591
2020-03-25 13:50:23,837 [dl_trainer.py:822] INFO Epoch 121, lr: 0.010000, val loss: 2.302585, val top-1 acc: 10.034678, top-5 acc: 50.023916
2020-03-25 13:50:35,532 [dl_trainer.py:732] WARNING [121][ 3040/   25][rank:0] loss: 2.303, average forward (0.004244) and backward (0.448826) time: 0.840935, iotime: 0.244472 
2020-03-25 13:50:41,951 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911363, Speed: 1123.592045 images/s
2020-03-25 13:50:41,952 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:50:41,952 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:50:43,647 [dl_trainer.py:634] INFO train iter: 3050, num_batches_per_epoch: 25
2020-03-25 13:50:43,647 [dl_trainer.py:635] INFO Epoch 122, avg train acc: 10.140625, lr: 0.010000, avg loss: 2.302587
2020-03-25 13:50:46,500 [dl_trainer.py:822] INFO Epoch 122, lr: 0.010000, val loss: 2.302585, val top-1 acc: 9.989836, top-5 acc: 50.023916
2020-03-25 13:51:03,927 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915624, Speed: 1118.363260 images/s
2020-03-25 13:51:03,928 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:51:03,928 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:51:06,431 [dl_trainer.py:634] INFO train iter: 3075, num_batches_per_epoch: 25
2020-03-25 13:51:06,431 [dl_trainer.py:635] INFO Epoch 123, avg train acc: 9.851562, lr: 0.001000, avg loss: 2.302594
2020-03-25 13:51:09,276 [dl_trainer.py:822] INFO Epoch 123, lr: 0.001000, val loss: 2.302586, val top-1 acc: 10.025710, top-5 acc: 50.014948
2020-03-25 13:51:13,068 [dl_trainer.py:732] WARNING [123][ 3080/   25][rank:0] loss: 2.303, average forward (0.004248) and backward (0.448472) time: 0.840451, iotime: 0.244565 
2020-03-25 13:51:25,799 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911258, Speed: 1123.720989 images/s
2020-03-25 13:51:25,800 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:51:25,800 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:51:29,080 [dl_trainer.py:634] INFO train iter: 3100, num_batches_per_epoch: 25
2020-03-25 13:51:29,081 [dl_trainer.py:635] INFO Epoch 124, avg train acc: 9.968750, lr: 0.001000, avg loss: 2.302597
2020-03-25 13:51:31,932 [dl_trainer.py:822] INFO Epoch 124, lr: 0.001000, val loss: 2.302586, val top-1 acc: 10.025710, top-5 acc: 50.014948
2020-03-25 13:51:47,670 [dl_trainer.py:732] WARNING [124][ 3120/   25][rank:0] loss: 2.303, average forward (0.004265) and backward (0.448420) time: 0.768920, iotime: 0.244251 
2020-03-25 13:51:47,763 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915147, Speed: 1118.946384 images/s
2020-03-25 13:51:47,764 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:51:47,764 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:51:51,855 [dl_trainer.py:634] INFO train iter: 3125, num_batches_per_epoch: 25
2020-03-25 13:51:51,855 [dl_trainer.py:635] INFO Epoch 125, avg train acc: 10.136719, lr: 0.001000, avg loss: 2.302578
2020-03-25 13:51:54,713 [dl_trainer.py:822] INFO Epoch 125, lr: 0.001000, val loss: 2.302585, val top-1 acc: 10.025710, top-5 acc: 50.014948
2020-03-25 13:52:09,668 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912667, Speed: 1121.986178 images/s
2020-03-25 13:52:09,669 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:52:09,669 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:52:14,528 [dl_trainer.py:634] INFO train iter: 3150, num_batches_per_epoch: 25
2020-03-25 13:52:14,528 [dl_trainer.py:635] INFO Epoch 126, avg train acc: 9.992188, lr: 0.001000, avg loss: 2.302582
2020-03-25 13:52:17,375 [dl_trainer.py:822] INFO Epoch 126, lr: 0.001000, val loss: 2.302586, val top-1 acc: 10.004783, top-5 acc: 50.014948
2020-03-25 13:52:25,135 [dl_trainer.py:732] WARNING [126][ 3160/   25][rank:0] loss: 2.303, average forward (0.004253) and backward (0.448514) time: 0.839700, iotime: 0.243603 
2020-03-25 13:52:31,553 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911823, Speed: 1123.024735 images/s
2020-03-25 13:52:31,554 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:52:31,554 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:52:37,197 [dl_trainer.py:634] INFO train iter: 3175, num_batches_per_epoch: 25
2020-03-25 13:52:37,197 [dl_trainer.py:635] INFO Epoch 127, avg train acc: 10.125000, lr: 0.001000, avg loss: 2.302587
2020-03-25 13:52:40,045 [dl_trainer.py:822] INFO Epoch 127, lr: 0.001000, val loss: 2.302585, val top-1 acc: 10.004783, top-5 acc: 50.044842
2020-03-25 13:52:53,455 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912553, Speed: 1122.126053 images/s
2020-03-25 13:52:53,456 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:52:53,456 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:52:59,774 [dl_trainer.py:732] WARNING [127][ 3200/   25][rank:0] loss: 2.303, average forward (0.004259) and backward (0.448490) time: 0.768769, iotime: 0.244347 
2020-03-25 13:52:59,905 [dl_trainer.py:634] INFO train iter: 3200, num_batches_per_epoch: 25
2020-03-25 13:52:59,905 [dl_trainer.py:635] INFO Epoch 128, avg train acc: 9.726562, lr: 0.001000, avg loss: 2.302598
2020-03-25 13:53:02,759 [dl_trainer.py:822] INFO Epoch 128, lr: 0.001000, val loss: 2.302586, val top-1 acc: 10.004783, top-5 acc: 50.044842
2020-03-25 13:53:15,426 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.915395, Speed: 1118.642976 images/s
2020-03-25 13:53:15,427 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:53:15,427 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:53:22,673 [dl_trainer.py:634] INFO train iter: 3225, num_batches_per_epoch: 25
2020-03-25 13:53:22,674 [dl_trainer.py:635] INFO Epoch 129, avg train acc: 9.921875, lr: 0.001000, avg loss: 2.302593
2020-03-25 13:53:25,523 [dl_trainer.py:822] INFO Epoch 129, lr: 0.001000, val loss: 2.302585, val top-1 acc: 10.004783, top-5 acc: 50.044842
2020-03-25 13:53:37,239 [dl_trainer.py:732] WARNING [129][ 3240/   25][rank:0] loss: 2.303, average forward (0.004263) and backward (0.448665) time: 0.838526, iotime: 0.242281 
2020-03-25 13:53:37,327 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.912509, Speed: 1122.181208 images/s
2020-03-25 13:53:37,328 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:53:37,328 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:53:45,318 [dl_trainer.py:634] INFO train iter: 3250, num_batches_per_epoch: 25
2020-03-25 13:53:45,318 [dl_trainer.py:635] INFO Epoch 130, avg train acc: 9.949219, lr: 0.001000, avg loss: 2.302594
2020-03-25 13:53:48,167 [dl_trainer.py:822] INFO Epoch 130, lr: 0.001000, val loss: 2.302585, val top-1 acc: 10.004783, top-5 acc: 50.044842
2020-03-25 13:53:59,198 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911254, Speed: 1123.726293 images/s
2020-03-25 13:53:59,199 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:53:59,199 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:54:08,012 [dl_trainer.py:634] INFO train iter: 3275, num_batches_per_epoch: 25
2020-03-25 13:54:08,012 [dl_trainer.py:635] INFO Epoch 131, avg train acc: 10.121094, lr: 0.001000, avg loss: 2.302586
2020-03-25 13:54:10,866 [dl_trainer.py:822] INFO Epoch 131, lr: 0.001000, val loss: 2.302586, val top-1 acc: 10.004783, top-5 acc: 50.000000
2020-03-25 13:54:14,652 [dl_trainer.py:732] WARNING [131][ 3280/   25][rank:0] loss: 2.303, average forward (0.004350) and backward (0.448831) time: 0.840236, iotime: 0.243707 
2020-03-25 13:54:21,060 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.910828, Speed: 1124.252093 images/s
2020-03-25 13:54:21,060 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:54:21,060 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:54:30,654 [dl_trainer.py:634] INFO train iter: 3300, num_batches_per_epoch: 25
2020-03-25 13:54:30,654 [dl_trainer.py:635] INFO Epoch 132, avg train acc: 9.992188, lr: 0.001000, avg loss: 2.302583
2020-03-25 13:54:33,507 [dl_trainer.py:822] INFO Epoch 132, lr: 0.001000, val loss: 2.302586, val top-1 acc: 10.004783, top-5 acc: 50.000000
2020-03-25 13:54:42,940 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911612, Speed: 1123.284999 images/s
2020-03-25 13:54:42,940 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:54:42,941 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:54:49,314 [dl_trainer.py:732] WARNING [132][ 3320/   25][rank:0] loss: 2.303, average forward (0.004258) and backward (0.448769) time: 0.770401, iotime: 0.245323 
2020-03-25 13:54:53,365 [dl_trainer.py:634] INFO train iter: 3325, num_batches_per_epoch: 25
2020-03-25 13:54:53,365 [dl_trainer.py:635] INFO Epoch 133, avg train acc: 9.980469, lr: 0.001000, avg loss: 2.302596
2020-03-25 13:54:56,219 [dl_trainer.py:822] INFO Epoch 133, lr: 0.001000, val loss: 2.302585, val top-1 acc: 10.004783, top-5 acc: 50.000000
2020-03-25 13:55:04,863 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.913424, Speed: 1121.056846 images/s
2020-03-25 13:55:04,864 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:55:04,864 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:55:16,060 [dl_trainer.py:634] INFO train iter: 3350, num_batches_per_epoch: 25
2020-03-25 13:55:16,061 [dl_trainer.py:635] INFO Epoch 134, avg train acc: 9.769531, lr: 0.001000, avg loss: 2.302590
2020-03-25 13:55:18,910 [dl_trainer.py:822] INFO Epoch 134, lr: 0.001000, val loss: 2.302585, val top-1 acc: 10.025710, top-5 acc: 50.029895
2020-03-25 13:55:26,670 [dl_trainer.py:732] WARNING [134][ 3360/   25][rank:0] loss: 2.303, average forward (0.004256) and backward (0.448533) time: 0.839021, iotime: 0.242914 
2020-03-25 13:55:26,745 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.911695, Speed: 1123.182211 images/s
2020-03-25 13:55:26,745 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:55:26,745 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
2020-03-25 13:55:38,745 [dl_trainer.py:634] INFO train iter: 3375, num_batches_per_epoch: 25
2020-03-25 13:55:38,746 [dl_trainer.py:635] INFO Epoch 135, avg train acc: 9.781250, lr: 0.001000, avg loss: 2.302583
2020-03-25 13:55:41,601 [dl_trainer.py:822] INFO Epoch 135, lr: 0.001000, val loss: 2.302585, val top-1 acc: 9.989836, top-5 acc: 50.000000
2020-03-25 13:55:48,685 [dist_trainer.py:90] WARNING Time per iteration including communication: 0.914122, Speed: 1120.199983 images/s
2020-03-25 13:55:48,686 [distributed_optimizer.py:142] INFO Average number of selected gradients: 269.000000, exact k: 269
2020-03-25 13:55:48,686 [distributed_optimizer.py:143] INFO The number of selected gradients: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
